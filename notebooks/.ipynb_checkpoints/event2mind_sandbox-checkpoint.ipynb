{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'November was a trying month… on the 7th Dante had a major accident. 5 minutes before school and he and some friends are climbing the fence, I tell him it’s not a good idea and to get down. I turn back to talk to Jodi (on of my best mom friend’s at the school) and Dante comes to me screaming with his hand full of blood. I run him into my classroom and get him to the sink, as I turn on the water to clean the area the flap of his thumb lifts away and I see the bone. Shit. This isn’t something I can fix here, I grab my first aid kit and wrap it like crazy because it’s bleeding like crazy. I phone James and tell him to get to the ER as Dante is screaming and freaking out in the background as I’m trying to usher him back to the car as he’s bleeding like a stuffed pig. Unfortunately in the ER I learned that my child doesn’t take to freezing, an hour of gel freezing and he still felt the 2 needles as they went in, 15 minutes later and he felt the last 2 stitches of 8. He needed more because his finger still had gaps, the doctor didn’t want to cause him anymore pain so he glued them. It was an intense and deep gash that spiraled all the way up his thumb. I was trying to stay strong for him but I did break down as he screamed and cried, I was left to emotionally drained that day. James was able to take the remainder of the day off and stay with him. He missed 2 more days of school and then had an extra long weekend due to the holiday and the pro day but for 2 weeks he couldn’t write (of course it was his right hand.) 3 doctor visits later and he finally got them out full last week, the first visit the doctor wanted them in longer because of the severity. 2nd time he could only get 6 out because the glue had gotten on the last 2 stitches and he didn’t want to have to dig them out so we had to soak and dissolve the glue for 3 days. 3rd time the last 2 came out.  Even now he’s slowly regaining his writing skills as there was some nerve damage.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'well, i tried to get an x-ray of my neck today but, when i got to the medical center and stood in line to wait to get checked in, i was told my doctor hadn’t sent over the orders for it! and the thing is he said he had already sent it when i asked him if i needed anything to get it done. i don’t like being lied to. so, since i have to go to the medical center thursday morning for a consultation for p/t, i’ll just go on back over and get the xray. the lady there said monday and tuesday are really busy days and thursday would be much better.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I can’t help but feel annoyed, angry, disheartened, let down… and yet in another way I want to say “you don’t deserve to know him.” Dante’s growing into such an amazing child and yet it seems our family dwindles like crazy, he brings up James’ sister “aunty Tammy” and asks why we never see her. I say she’s busy because she has 2 of her own little boys, but that’s not the case. James’ sister had this dream of being an amazing aunt to  Dante and she has done nothing to be in his life. Birthday gifts, Christmas, there’s no communication or her ever asking about him, not that I even speak to  her much but she just doesn’t care to be an active part in his world which pisses me off to no end. James’ mother couldn’t get herself clean to stay in his life… she’s non existent to him. It boils my blood because when he was born she was so proud, got clean for a while, and then couldn’t hack it (ended up visiting and left her morphine out where our very smart 2 year old brought us a handful of pills and asked if they  were candy.) That was the last time she saw him and her memory has since been forgotten. You couldn’t even get clean to be in your grandchild’s life? She was always a pathetic excuse for a mother, James’ childhood simply enrages me, the idea of a child living the way he did because of her ways makes me sick. She doesn’t deserve to know my child. My own brother “uncle Jason” is seen in passing about 5 times a year, he’s good with Dante, pleasant enough considering my brother has so many anger issues. He’s also a drug addict so it’s not like I would ever allow him time alone with Dante, not that he’d ever want to spend time with him. Christmas is coming up and in a way it’s bittersweet. My half cousin Tianna’s two girls (Stella and Piper)  have two sets of everything, tons of aunts and uncles, and they have a huge loving family unit. Dante doesn’t have that, yes his grandparents love him like crazy but his family connection is like mine. When I was young I only had one set of grandparents, my dad was adopted and his mother wasn’t around at all… in a way my grandparents adopted him as well when he married my young at the age of 18. I had my uncle George and my Omi and Opi and my mom and dad and my brother. I remember when George met Denise and I met Tianna (Denise’s child from her first marriage.) I  remember the day that I learned that George had proposed to Denise and that they were getting married, I cried and my mom thought I was happy. I wasn’t happy, I was devastated that suddenly I had to share my family (horrible to thing to cry about right?) Tianna already had 2 sets of grandparents, she had tons of aunts, and now she was getting my uncle whom I loved and thought was the coolest guy around as a dad. I was so angry. I never really got to  meet Tianna’s dad’s side of the family, I met her grandparents a few times but they never remembered my name which really hurt and annoyed me. I joined soccer and Tianna’s dad was the coach, he wasn’t nice to me which drove the wedge deeper. It hurts… it hurts that my child has the same issue that I did although I really don’t think he’s realized that he’s different. His “grandpa Morgan” isn’t his real grandpa, more like a man who took on his father and tried to “raise” him to be a man, he obviously didn’t stay with James’ mom but he’s still in our life and I’m grateful  that he’s there. Unfortunately he’s not around much, we see him 2-3 times a year because he lives in Golden. Uncle Adam is James’ childhood best friend, a good guy  who’s more of a businessman who  lives to the beat of his own drum and wouldn’t know what to do with  a child if his  life depended on it. He’s around but again it’s only a few times a year when he visits from Calgary. I want to say sometimes you get to choose your own family, but even the family I chose for him and thought would be around forever, the people who were there when he was born, grew, shared so many moments with are no longer around. They don’t seem to care either, it’s not like “aunty Kat” ever talks to me or asks about him. Seems like moving meant the end of our friendship and our “family ties.”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = textacy.preprocess.preprocess_text(text, fix_unicode=True, no_contractions=True, no_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract People"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a good day DATE\n",
      "today DATE\n",
      "the other half CARDINAL\n",
      "three CARDINAL\n",
      "Jessie PERSON\n",
      "one CARDINAL\n",
      "Mutual Friend ORG\n",
      "three CARDINAL\n",
      "Jessie PERSON\n",
      "one CARDINAL\n",
      "Jessie PERSON\n",
      "first ORDINAL\n",
      "last night TIME\n",
      "Femmes PERSON\n",
      "January DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Femmes', 'Jessie'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-6dd76d7f4266>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-6dd76d7f4266>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def resolved\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "textacy.text_utils.keyword_in_context(doc.text, 'Christmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie nsubj bringing\n",
      "Jessie nsubj liked\n",
      "Jessie pobj about\n",
      "Femmes nsubj play\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        token = ent.root\n",
    "        print(ent.text, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:32:32 - INFO - allennlp.models.archival -   loading archive file data/coref-model.tar.gz\n",
      "12/06/2018 22:32:32 - INFO - allennlp.models.archival -   extracting archive file data/coref-model.tar.gz to temp dir /tmp/tmp2wg5xsym\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 22:32:32 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp2wg5xsym/vocabulary.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'type': 'coref'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.type = coref\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.coreference_resolution.coref.CoreferenceResolver'> from params {'antecedent_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 3680, 'num_layers': 2}, 'context_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'feature_size': 20, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]], 'lexical_dropout': 0.5, 'max_antecedents': 150, 'max_span_width': 10, 'mention_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1220, 'num_layers': 2}, 'spans_per_word': 0.4, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = 262\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'} and extras {}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100} and extras {}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 100\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [5]\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 300\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5103966d8>}\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.type = lstm\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.stateful = False\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.bidirectional = True\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.dropout = 0.2\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.hidden_size = 200\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.input_size = 400\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.num_layers = 1\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.context_layer.batch_first = True\n",
      "/home/russell/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.mention_feedforward.input_dim = 1220\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.mention_feedforward.num_layers = 2\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.mention_feedforward.hidden_dims = 150\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.mention_feedforward.activations = relu\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.mention_feedforward.dropout = 0.2\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.antecedent_feedforward.input_dim = 3680\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.antecedent_feedforward.num_layers = 2\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.antecedent_feedforward.hidden_dims = 150\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.antecedent_feedforward.activations = relu\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.antecedent_feedforward.dropout = 0.2\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.feature_size = 20\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.max_span_width = 10\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.spans_per_word = 0.4\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.max_antecedents = 150\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.lexical_dropout = 0.5\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer = [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*scorer._module.*weight', {'type': 'xavier_normal'}], ['.*_global_attention._module.weight', {'type': 'xavier_normal'}], ['_distance_embedding.weight', {'type': 'xavier_normal'}], ['_span_width_embedding.weight', {'type': 'xavier_normal'}], ['_context_layer._module.weight_ih.*', {'type': 'xavier_normal'}], ['_context_layer._module.weight_hh.*', {'type': 'orthogonal'}]]\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:32:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0 using _context_layer._module.weight_ih.* intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0 using _context_layer._module.weight_hh.* intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_ih_l0_reverse using _context_layer._module.weight_ih.* intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _context_layer._module.weight_hh_l0_reverse using _context_layer._module.weight_hh.* intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _antecedent_scorer._module.weight using .*scorer._module.*weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _attentive_span_extractor._global_attention._module.weight using .*_global_attention._module.weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Initializing _distance_embedding.weight using _distance_embedding.weight intitializer\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.0.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _antecedent_feedforward._module._linear_layers.1.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _antecedent_scorer._module.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _attentive_span_extractor._global_attention._module.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_hh_l0_reverse\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _context_layer._module.bias_ih_l0_reverse\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _mention_pruner._scorer.1._module.weight\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "12/06/2018 22:32:32 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}, 'type': 'coref'} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.type = coref\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.coreference_resolution.conll.ConllCorefReader'> from params {'max_span_width': 10, 'token_indexers': {'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': False, 'type': 'single_id'}}} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.max_span_width = 10\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'type': 'characters'} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer'> from params {} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'lowercase_tokens': False, 'type': 'single_id'} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'lowercase_tokens': False} and extras {}\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 22:32:33 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/coref-model.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref = predictor.predict(document = doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, I, my, my, me, I, I, my, my, I, my, my, I, my, I, I, I, I, I, I, me, me, I, I, me, I, I, me, I, I, I, I, I, me, I, me, my, I, I, I, I, I, I, I, I, I, I, I, I]\n",
      "[we, our, We, our, we, our, we, we, we, us]\n",
      "[my girl dog, she, her, She, she, she, she, She, she, she]\n",
      "[beat, This]\n",
      "[a cute new collar tho, it, it]\n",
      "[my teeth, they, they]\n",
      "[three more trays, the trays]\n",
      "[Invisalign, YAY]\n",
      "[Jessie, Jessie, she, she, she, Jessie, her, she, her, she, her, her, her]\n",
      "[three artists, them]\n",
      "[our collective, the collective]\n",
      "[the festival, this festival, the festival]\n",
      "[them, them]\n",
      "[this, it]\n"
     ]
    }
   ],
   "source": [
    "for cluster in coref['clusters']:\n",
    "    spans = [doc[first:last+1] for first, last in cluster]\n",
    "    print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So [I(0)] have had a good day today . [I(0)] found out [we(1)] got the other half of [our(1)] funding for [my(0)] travel grant , which paid for [my(0)] friend to come with [me(0)] . So that 's good , she and [I(0)] will both get some money back . [I(0)] took [my(0)] dogs to the pet store so [my girl dog(2)] could get a new collar , but [she(2)] wanted to [beat(3)] everyone up . [This(3)] is an ongoing issue with [her(2)] . [She(2)] 's so little and cute too but damn [she(2)] acts like [she(2)] 's gon na go for the jugular with everyone [she(2)] does not know ! [She(2)] did end up with [a cute new collar tho(4)] , [it(4)] has pineapples on [it(4)] . [I(0)] went to the dentist and [she(2)] 's happy with [my(0)] [Invisalign(7)] progress . [We(1)] have [three more trays(6)] and then [she(2)] does an impression to make sure [my teeth(5)] are where [they(5)] need to be before [they(5)] get the rest of [the trays(6)] . [YAY(7)] ! And [I(0)] do not have to make another payment until closer to the end of [my(0)] treatment . [I(0)] had some work emails with [the festival(11)] , and [Jessie(8)] was bringing up some important points , and one of [our(1)] potential artists was too expensive to work with , so Mutual Friend was asking for names for some other people [we(1)] could work with . So [I(0)] suggested like , [three artists(9)] , and [Jessie(8)] actually liked the idea of one of [them(9)] doing it . Which is nice . [I(0)] notice [she(8)] is very encouraging at whatever [I(0)] contribute to [our collective(10)] . It 's sweet . [I(0)] kind of know this is like , the only link [we(1)] have with each other right now besides social media , so it seems like [she(8)] 's trying to make sure [I(0)] know [she(8)] still wants [me(0)] to be involved and does not have bad feelings for [me(0)] . And there was a short period when [I(0)] was seriously thinking of leaving [the collective(10)] and not working with [this festival(11)] anymore . [I(0)] was so sad , and felt so upset , and did not know what to do about [Jessie(8)] . It felt really close to [me(0)] throwing in the towel . But [I(0)] hung on through [the festival(11)] and it does not seem so bad from this viewpoint now with more time that has passed . And [we(1)] have been gentle , if reserved , with each other . [I(0)] mean [her(8)] last personal email to [me(0)] however many weeks ago was n't very nice . But it seems like [we(1)] have been able to put it aside for work reasons . [I(0)] dunno . [I(0)] still feel like if anything was gon na get mended between [us(1)] , [she(8)] would need to make the first moves on that . [I(0)] really do not want to try reaching out and get rejected even as a friend again . [I(0)] miss [her(8)] though . And sometimes [I(0)] think [she(8)] misses [me(0)] . But [I(0)] do not want to approach [her(8)] assuming we both miss each other and have [her(8)] turn it on [me(0)] again and make out like all these things are all in [my(0)] head . [I(0)] do not know about that butch [I(0)] went on a date with last night . [I(0)] feel more of a friend vibe from [her(8)] , than a romantic one . [I(0)] can not help it , [I(0)] am just not attracted to butches . And [I(0)] do not know how to flirt with [them(12)] . And [I(0)] do not think of [them(12)] in a sexy way . But [I(0)] WOULD like another butch buddy . [I(0)] mean yeah maybe Femmes do play games , or maybe [I(0)] just chased all the wrong Femmes . Maybe [I(0)] will just leave [this(13)] and not think about [it(13)] much until [I(0)] get back to town in January .\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coref_resolved(doc, coref):\n",
    "    resolved = [token.text for token in doc]\n",
    "    for token in doc:\n",
    "        cluster_n = 0\n",
    "        for cluster in coref['clusters']:\n",
    "            for first, last in cluster:\n",
    "                span = doc[first:last+1]\n",
    "                if first == last:\n",
    "                    resolved[first] = '[' + doc[first].text + '(' + str(cluster_n) + ')]'\n",
    "                else:\n",
    "                    resolved[first] = '[' + doc[first].text\n",
    "                    resolved[last] = doc[last].text + '(' + str(cluster_n) + ')]'\n",
    "\n",
    "            cluster_n += 1\n",
    "    return ' '.join(resolved)\n",
    "coref_resolved(doc, coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralCoref\n",
    "(maybe not quite as good? maybe it is. certainly easier.)\n",
    "\n",
    "(I think I'll use this together with named entity recognititon to ID unique people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[we: [we, our],\n",
       " my friend: [my friend, she, she, her, She, she, she, she, She, she, she],\n",
       " a cute new collar tho: [a cute new collar tho, it, it],\n",
       " We: [We, our, we],\n",
       " three more trays: [three more trays, the trays],\n",
       " they: [they, they],\n",
       " Jessie: [Jessie, Jessie, she, she, she, her, she, her, she, her, her, her],\n",
       " three artists: [three artists, them],\n",
       " our: [our, we, we, us],\n",
       " me: [me, me, me],\n",
       " this festival: [this festival, the festival],\n",
       " It: [It, it],\n",
       " them: [them, them],\n",
       " this: [this, it]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that's good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She's so little and cute too but damn she acts like she's gonna go for the jugular with everyone she does not know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she's happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I do not have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It's sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she's trying to make sure I know she still wants me to be involved and does not have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and did not know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it does not seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn't very nice. But it seems like we have been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really do not want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I do not want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I do not know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can not help it, I am just not attracted to butches. And I do not know how to flirt with them. And I do not think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I will just leave this and not think about it much until I get back to town in January.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So I have had a good day today. I found out we got the other half of we funding for my travel grant, which paid for my friend to come with me. So that's good, my friend and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but my friend wanted to beat everyone up. This is an ongoing issue with my friend. my friend's so little and cute too but damn my friend acts like my friend's gonna go for the jugular with everyone my friend does not know! my friend did end up with a cute new collar tho, a cute new collar tho has pineapples on a cute new collar tho. I went to the dentist and my friend's happy with my Invisalign progress. We have three more trays and then my friend does an impression to make sure my teeth are where they need to be before they get the rest of three more trays. YAY! And I do not have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of We potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people We could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of three artists doing it. Which is nice. I notice Jessie is very encouraging at whatever I contribute to our collective. It's sweet. I kind of know this is like, the only link our have with each other right now besides social media, so it seems like Jessie's trying to make sure I know Jessie still wants me to be involved and does not have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and did not know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through this festival and It does not seem so bad from this viewpoint now with more time that has passed. And our have been gentle, if reserved, with each other. I mean Jessie last personal email to me however many weeks ago wasn't very nice. But it seems like we have been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between our, Jessie would need to make the first moves on that. I really do not want to try reaching out and get rejected even as a friend again. I miss Jessie though. And sometimes I think Jessie misses me. But I do not want to approach Jessie assuming we both miss each other and have Jessie turn it on me again and make out like all these things are all in my head. I do not know about that butch I went on a date with last night. I feel more of a friend vibe from Jessie, than a romantic one. I can not help it, I am just not attracted to butches. And I do not know how to flirt with them. And I do not think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I will just leave this and not think about this much until I get back to town in January.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NEM & Coref to ID unique people\n",
    "NEM can tell which clusters are people\n",
    "NEM can give clusters better names\n",
    "NEM can link clusters together\n",
    "NEM can tell whether a cluster contains a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Femmes', 'Jessie'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = set([ent.text for ent in doc.ents if ent.label_ == 'PERSON'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[we: [we, our],\n",
       " my friend: [my friend, she, she, her, She, she, she, she, She, she, she],\n",
       " a cute new collar tho: [a cute new collar tho, it, it],\n",
       " We: [We, our, we],\n",
       " three more trays: [three more trays, the trays],\n",
       " they: [they, they],\n",
       " Jessie: [Jessie, Jessie, she, she, she, her, she, her, she, her, her, her],\n",
       " three artists: [three artists, them],\n",
       " our: [our, we, we, us],\n",
       " me: [me, me, me],\n",
       " this festival: [this festival, the festival],\n",
       " It: [It, it],\n",
       " them: [them, them],\n",
       " this: [this, it]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, I, my, my, me, I, I, my, my, I, my, my, I, my, I, I, I, I, I, I, me, me, I, I, me, I, I, me, I, I, I, I, I, me, I, me, my, I, I, I, I, I, I, I, I, I, I, I, I]\n",
      "[we, our, We, our, we, our, we, we, we, us]\n",
      "[my girl dog, she, her, She, she, she, she, She, she, she]\n",
      "[beat, This]\n",
      "[a cute new collar tho, it, it]\n",
      "[my teeth, they, they]\n",
      "[three more trays, the trays]\n",
      "[Invisalign, YAY]\n",
      "[Jessie, Jessie, she, she, she, Jessie, her, she, her, she, her, her, her]\n",
      "[three artists, them]\n",
      "[our collective, the collective]\n",
      "[the festival, this festival, the festival]\n",
      "[them, them]\n",
      "[this, it]\n"
     ]
    }
   ],
   "source": [
    "for cluster in coref['clusters']:\n",
    "    spans = [doc[first:last+1] for first, last in cluster]\n",
    "    print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en_coref_md.neuralcoref.neuralcoref.Cluster"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc._.coref_clusters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Subject Verb Object Triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - want - to say\n",
      "you - don’t deserve - to know\n",
      "he - brings - James’ sister\n",
      "he - brings - ”\n",
      "we - never see - her\n",
      "she - has - 2\n",
      "that - ’s not - case\n",
      "sister - had - dream\n",
      "she - has done - nothing\n",
      "there - ’s - communication\n",
      "there - ’s - asking\n",
      "I - speak - much\n",
      "she - doesn’t care - to be\n",
      "which - pisses - me\n",
      "It - boils - blood\n",
      "old - brought - us\n",
      "old - brought - handful\n",
      "they - were - candy\n",
      "That - was - time\n",
      "she - saw - him\n",
      "You - get - to be\n",
      "She - was - excuse\n",
      "childhood - enrages - me\n",
      "She - doesn’t deserve - to know\n",
      "brother - has - anger issues\n",
      "He - ’s - drug addict\n",
      "he - want - to spend\n",
      "they - have - family unit\n",
      "Dante - doesn’t have - that\n",
      "grandparents - love - him\n",
      "I - had - set\n",
      "grandparents - adopted - him\n",
      "he - married - young\n",
      "I - had - uncle\n",
      "George - met - Denise\n",
      "I - met - Tianna\n",
      "I - remember - day\n",
      "I - had - to share\n",
      "Tianna - had - sets\n",
      "she - had - tons\n",
      "uncle - was - guy\n",
      "I - got - meet\n",
      "I - met - grandparents\n",
      "they - never remembered - name\n",
      "I - joined - soccer\n",
      "I - joined - dad\n",
      "which - drove - wedge\n",
      "child - has - issue\n",
      "” - isn’t - grandpa\n",
      "who - took - father\n",
      "him - to be - man\n",
      "we - see - him\n",
      "Uncle Adam - is - friend\n",
      "I - want - to say\n",
      "you - get - to choose\n",
      "They - don’t seem - to care\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - want - to say\n",
      "you - don’t deserve - to know\n",
      "he - brings - James’ sister\n",
      "he - brings - ”\n",
      "we - never see - her\n",
      "she - has - 2\n",
      "that - ’s not - case\n",
      "sister - had - dream\n",
      "she - has done - nothing\n",
      "there - ’s - communication\n",
      "there - ’s - asking\n",
      "I - speak - much\n",
      "she - doesn’t care - to be\n",
      "which - pisses - me\n",
      "It - boils - blood\n",
      "old - brought - us\n",
      "old - brought - handful\n",
      "they - were - candy\n",
      "That - was - time\n",
      "she - saw - him\n",
      "You - get - to be\n",
      "She - was - excuse\n",
      "childhood - enrages - me\n",
      "She - doesn’t deserve - to know\n",
      "brother - has - anger issues\n",
      "He - ’s - drug addict\n",
      "he - want - to spend\n",
      "they - have - family unit\n",
      "Dante - doesn’t have - that\n",
      "grandparents - love - him\n",
      "I - had - set\n",
      "grandparents - adopted - him\n",
      "he - married - young\n",
      "I - had - uncle\n",
      "George - met - Denise\n",
      "I - met - Tianna\n",
      "I - remember - day\n",
      "I - had - to share\n",
      "Tianna - had - sets\n",
      "she - had - tons\n",
      "uncle - was - guy\n",
      "I - got - meet\n",
      "I - met - grandparents\n",
      "they - never remembered - name\n",
      "I - joined - soccer\n",
      "I - joined - dad\n",
      "which - drove - wedge\n",
      "child - has - issue\n",
      "” - isn’t - grandpa\n",
      "who - took - father\n",
      "him - to be - man\n",
      "we - see - him\n",
      "Uncle Adam - is - friend\n",
      "I - want - to say\n",
      "you - get - to choose\n",
      "They - don’t seem - to care\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for subj, verb, obj in svo_triples:\n",
    "    subj_phrase = ' '.join([token.text for token in subj.root.subtree])\n",
    "    obj_phrase = ' '.join([token.text for token in obj.root.subtree])\n",
    "#     start, end = textacy.spacier.utils.get_span_for_verb_auxiliaries(verb.root)\n",
    "#     verb_phrase = doc[start:end+1]\n",
    "    print(subj, '-', verb, '-', obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Semistructured Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I was so sad, and felt so upset, and did not to what to do about jessie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.'\n",
    "preprocessed = textacy.preprocess.preprocess_text(text, fix_unicode=True, no_contractions=True, no_accents=True)\n",
    "doc = nlp(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that's good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She's so little and cute too but damn she acts like she's gonna go for the jugular with everyone she does not know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she's happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I do not have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It's sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she's trying to make sure I know she still wants me to be involved and does not have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and did not know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it does not seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn't very nice. But it seems like we have been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really do not want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I do not want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I do not know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can not help it, I am just not attracted to butches. And I do not know how to flirt with them. And I do not think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I will just leave this and not think about it much until I get back to town in January.\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[was, felt, do]\n",
      "['be', 'feel', 'do']\n"
     ]
    }
   ],
   "source": [
    "verbs = textacy.spacier.utils.get_main_verbs_of_sent([sent for sent in doc.sents][0])\n",
    "print(verbs)\n",
    "verb_lemmas = [verb.lemma_ for verb in verbs]\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "I - was - so sad, and felt so upset, and did not to what to do about jessie\n",
      "feel\n",
      "I - was - so sad, and felt so upset, and did not to what to do about jessie\n",
      "do\n",
      "I - was - so sad, and felt so upset, and did not to what to do about jessie\n"
     ]
    }
   ],
   "source": [
    "for verb_lemma in verb_lemmas:\n",
    "    print(verb_lemma)\n",
    "    subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "    for subject in subjects:\n",
    "        statements = textacy.extract.semistructured_statements(doc, subject.text, verb.lemma_)\n",
    "        for entity, cue, fragment in statements:\n",
    "            print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I, was, so sad, and felt so upset, and did not to what to do about jessie)\n"
     ]
    }
   ],
   "source": [
    "for statement in textacy.extract.semistructured_statements(doc, 'I', cue='be'):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement in textacy.extract.semistructured_statements(doc, 'I', cue='felt'):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (entity, cue, fragment)\n",
    "statements = textacy.extract.semistructured_statements(doc, 'I', cue='feel')\n",
    "\n",
    "for entity, cue, fragment in statements:\n",
    "    print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get cues\n",
    "all_statements = []\n",
    "for sent in doc.sents:\n",
    "    verbs = textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "#     print('sent:', sent, '\\nverbs:', verbs)\n",
    "    for verb in verbs:\n",
    "        objects = textacy.spacier.utils.get_objects_of_verb(verb)\n",
    "        subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "        for subject in subjects:\n",
    "            statements = textacy.extract.semistructured_statements(doc, subject.text, verb.lemma_)\n",
    "            for statement in statements:\n",
    "#                 print(subject, verb, statement)\n",
    "                all_statements += [statement]\n",
    "    \n",
    "    print('\\n')\n",
    "for entity, cue, fragment in set(all_statements):\n",
    "    print(entity, '-', cue, '-', fragment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllenNLP OIE\n",
    "(meh, doesn't seem to outperform extract_semistructured?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:43:23 - INFO - allennlp.models.archival -   loading archive file data/openie-model.tar.gz\n",
      "12/06/2018 22:43:23 - INFO - allennlp.models.archival -   extracting archive file data/openie-model.tar.gz to temp dir /tmp/tmp2o1p6ybf\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 22:43:23 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp2o1p6ybf/vocabulary.\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 22:43:23 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fa5482169b0>}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/06/2018 22:43:24 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7fa5103d0630>\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/06/2018 22:43:24 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_archive('data/openie-model.tar.gz')\n",
    "oie_predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sent in doc.sents:\n",
    "    predictions += [oie_predictor.predict(sentence=sent.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': \"'s\",\n",
       "   'description': \"and [ARG0: she] [V: 's] [ARG1: happy with my Invisalign progress] .\",\n",
       "   'tags': ['O',\n",
       "    'B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']}],\n",
       " 'words': ['and',\n",
       "  'she',\n",
       "  \"'s\",\n",
       "  'happy',\n",
       "  'with',\n",
       "  'my',\n",
       "  'Invisalign',\n",
       "  'progress',\n",
       "  '.']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'decided',\n",
       "   'description': '[ARG0: John] [V: decided] [ARG1: to run for office next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']},\n",
       "  {'verb': 'run',\n",
       "   'description': '[ARG0: John] [BV: decided to] [V: run] [ARG1: for office] [ARG2: next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-BV',\n",
       "    'I-BV',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-ARG2',\n",
       "    'I-ARG2',\n",
       "    'O']}],\n",
       " 'words': ['John',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'run',\n",
       "  'for',\n",
       "  'office',\n",
       "  'next',\n",
       "  'month',\n",
       "  '.']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\n",
    "  sentence=\"John decided to run for office next month.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:16:45 - INFO - allennlp.common.file_utils -   https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz not found in cache, downloading to /tmp/tmpzzo9jnen\n",
      "100%|██████████| 65722182/65722182 [00:18<00:00, 3518115.57B/s]\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   copying /tmp/tmpzzo9jnen to cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   creating metadata file for /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.common.file_utils -   removing temp file /tmp/tmpzzo9jnen\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz from cache at /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd\n",
      "12/06/2018 21:17:04 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/dd04ba717be48bea13525e4293a243477876cdb0f0166abb8b09b5ed2e17cb3e.d68991c3e6de7fbcb5cf3e605d0e298f12cb857ca9d70aa8683abc886aa49edd to temp dir /tmp/tmpxls8z_09\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 21:17:05 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpxls8z_09/vocabulary.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}, 'type': 'srl'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.type = srl\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.semantic_role_labeler.SemanticRoleLabeler'> from params {'binary_feature_dim': 100, 'encoder': {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True}, 'initializer': [['tag_projection_layer.*weight', {'type': 'orthogonal'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': True, 'type': 'embedding'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 100\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 300, 'input_size': 200, 'num_layers': 8, 'recurrent_dropout_probability': 0.1, 'type': 'alternating_lstm', 'use_highway': True} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcab0068240>}\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.type = alternating_lstm\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.hidden_size = 300\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.input_size = 200\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.num_layers = 8\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.recurrent_dropout_probability = 0.1\n",
      "12/06/2018 21:17:05 - INFO - allennlp.common.params -   model.encoder.use_highway = True\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.binary_feature_dim = 100\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.embedding_dropout = 0.0\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer = [['tag_projection_layer.*weight', {'type': 'orthogonal'}]]\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.initializer.list.list.type = orthogonal\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.label_smoothing = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   model.ignore_span_metric = False\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight intitializer\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      binary_feature_embedding.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_0.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_1.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_2.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_3.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_4.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_5.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_6.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.input_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      encoder._module.layer_7.state_linearity.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "12/06/2018 21:17:06 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'srl'} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.type = srl\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {} and extras {}\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7fcab0065b00>\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.domain_identifier = None\n",
      "12/06/2018 21:17:06 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'decided',\n",
       "   'description': '[ARG0: John] [V: decided] [ARG1: to run for office next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O']},\n",
       "  {'verb': 'run',\n",
       "   'description': '[ARG0: John] [BV: decided to] [V: run] [ARG1: for office] [ARG2: next month] .',\n",
       "   'tags': ['B-ARG0',\n",
       "    'B-BV',\n",
       "    'I-BV',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-ARG2',\n",
       "    'I-ARG2',\n",
       "    'O']}],\n",
       " 'words': ['John',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'run',\n",
       "  'for',\n",
       "  'office',\n",
       "  'next',\n",
       "  'month',\n",
       "  '.']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/openie-model.2018-08-20.tar.gz\")\n",
    "predictor.predict(\n",
    "  sentence=\"John decided to run for office next month.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz from cache at /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8\n",
      "12/06/2018 20:43:08 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8 to temp dir /tmp/tmpcf1zn45w\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   type = default\n",
      "12/06/2018 20:43:14 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpcf1zn45w/vocabulary.\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'decomposable_attention', 'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.type = decomposable_attention\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.decomposable_attention.DecomposableAttention'> from params {'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'similarity_function': {'type': 'dot_product'}, 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'type': 'elmo_token_embedder', 'do_layer_norm': False, 'dropout': 0.2, 'options_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fcaafe315c0>}\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.options_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpcf1zn45w/fta/model.text_field_embedder.elmo.weight_file\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0.2\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "12/06/2018 20:43:14 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64bb115feb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mfrom_path\u001b[0;34m(cls, archive_path, predictor_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mPredictor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/archival.py\u001b[0m in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    151\u001b[0m                        \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                        \u001b[0mserialization_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                        cuda_device=cuda_device)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# This allows subclasses of Model to override _load.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(cls, config, serialization_dir, weights_file, cuda_device)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# want the code to look for it, so we remove it from the parameters here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mremove_pretrained_embedding_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# This class has a constructor, so create kwargs for it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mcreate_kwargs\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;31m# Not optional and not supplied, that's an error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0membedder_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mtoken_embedders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenEmbedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, params, **extras)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtakes_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# This is not a base class, so convert our params and extras into a dict of kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(cls, vocab, params)\u001b[0m\n\u001b[1;32m    124\u001b[0m                    \u001b[0mprojection_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                    \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                    scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/token_embedders/elmo_token_embedder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, do_layer_norm, dropout, requires_grad, projection_dim, vocab_to_cache, scalar_mix_parameters)\u001b[0m\n\u001b[1;32m     63\u001b[0m                           \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                           \u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                           scalar_mix_parameters=scalar_mix_parameters)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, num_output_representations, requires_grad, do_layer_norm, dropout, vocab_to_cache, keep_sentence_boundaries, scalar_mix_parameters, module)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         vocab_to_cache=vocab_to_cache)\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_cached_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_to_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_sentence_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_sentence_boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options_file, weight_file, requires_grad, vocab_to_cache)\u001b[0m\n\u001b[1;32m    549\u001b[0m                                    \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                    \u001b[0mstate_projection_clip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proj_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                    requires_grad=requires_grad)\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elmo_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Number of representation layers including context independent layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/elmo_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, num_layers, requires_grad, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                     \u001b[0mrecurrent_dropout_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                     \u001b[0mmemory_cell_clip_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                     state_projection_clip_value)\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mlstm_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, cell_size, go_forward, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Additional projection matrix for making the hidden state smaller.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/modules/lstm_cell_with_projection.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Use sensible default initializations for parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mblock_orthogonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_linearity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/allennlp/nn/initializers.py\u001b[0m in \u001b[0;36mblock_orthogonal\u001b[0;34m(tensor, split_sizes, gain)\u001b[0m\n\u001b[1;32m    146\u001b[0m         block_slice = tuple([slice(start_index, start_index + step)\n\u001b[1;32m    147\u001b[0m                              for start_index, step in index_and_step_tuples])\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morthogonal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_logits': [-3.391864776611328, 4.570619106292725, 0.9505535364151001],\n",
       " 'label_probs': [0.00033908773912116885,\n",
       "  0.9735872745513916,\n",
       "  0.02607356198132038],\n",
       " 'h2p_attention': [[0.6615484952926636,\n",
       "   0.03999358043074608,\n",
       "   0.04555582255125046,\n",
       "   0.046556826680898666,\n",
       "   0.032376978546381,\n",
       "   0.02884303592145443,\n",
       "   0.021681087091565132,\n",
       "   0.02199293114244938,\n",
       "   0.021933946758508682,\n",
       "   0.03280186280608177,\n",
       "   0.02464543841779232,\n",
       "   0.0220700204372406],\n",
       "  [2.6478128347662278e-05,\n",
       "   0.9997804164886475,\n",
       "   2.5384990294696763e-05,\n",
       "   2.803125425998587e-05,\n",
       "   1.5035763681225944e-05,\n",
       "   1.5200890629785135e-05,\n",
       "   2.0365438103908673e-05,\n",
       "   1.5323941624956205e-05,\n",
       "   1.667179458308965e-05,\n",
       "   3.005035614478402e-05,\n",
       "   1.3499801752914209e-05,\n",
       "   1.3597185898106545e-05],\n",
       "  [0.10203886777162552,\n",
       "   0.08704567700624466,\n",
       "   0.11516872048377991,\n",
       "   0.11659414321184158,\n",
       "   0.0897686704993248,\n",
       "   0.1088109016418457,\n",
       "   0.06201941892504692,\n",
       "   0.06663387268781662,\n",
       "   0.06219317018985748,\n",
       "   0.07818535715341568,\n",
       "   0.0593428835272789,\n",
       "   0.052198376506567],\n",
       "  [0.0004895933088846505,\n",
       "   0.0005661703762598336,\n",
       "   0.0014663741458207369,\n",
       "   0.9835996627807617,\n",
       "   0.003986356779932976,\n",
       "   0.0008538846741430461,\n",
       "   0.0008042860426940024,\n",
       "   0.004030915908515453,\n",
       "   0.000624869717285037,\n",
       "   0.002379189943894744,\n",
       "   0.0006751827313564718,\n",
       "   0.0005234954296611249],\n",
       "  [0.02180355414748192,\n",
       "   0.016224386170506477,\n",
       "   0.04582870751619339,\n",
       "   0.32416868209838867,\n",
       "   0.1262902319431305,\n",
       "   0.21321798861026764,\n",
       "   0.03501936420798302,\n",
       "   0.09745176136493683,\n",
       "   0.036249496042728424,\n",
       "   0.04630831629037857,\n",
       "   0.022747211158275604,\n",
       "   0.014690198004245758],\n",
       "  [0.01745988056063652,\n",
       "   0.012007005512714386,\n",
       "   0.03334576636552811,\n",
       "   0.23861074447631836,\n",
       "   0.06420435011386871,\n",
       "   0.42332977056503296,\n",
       "   0.03817636892199516,\n",
       "   0.050691068172454834,\n",
       "   0.05518278479576111,\n",
       "   0.03998453542590141,\n",
       "   0.016478458419442177,\n",
       "   0.010529308579862118],\n",
       "  [0.010657047852873802,\n",
       "   0.011342254467308521,\n",
       "   0.013355554081499577,\n",
       "   0.39848199486732483,\n",
       "   0.026157716289162636,\n",
       "   0.050147585570812225,\n",
       "   0.36920735239982605,\n",
       "   0.02936599962413311,\n",
       "   0.03243587911128998,\n",
       "   0.04311087727546692,\n",
       "   0.008368291892111301,\n",
       "   0.00736935855820775],\n",
       "  [0.012434241361916065,\n",
       "   0.013259582221508026,\n",
       "   0.03426486626267433,\n",
       "   0.24050143361091614,\n",
       "   0.13842594623565674,\n",
       "   0.37977859377861023,\n",
       "   0.06643011420965195,\n",
       "   0.05420059710741043,\n",
       "   0.014995560981333256,\n",
       "   0.025156904011964798,\n",
       "   0.01212186086922884,\n",
       "   0.008430331014096737],\n",
       "  [0.025600627064704895,\n",
       "   0.02177058905363083,\n",
       "   0.05209308862686157,\n",
       "   0.08083993941545486,\n",
       "   0.0719628632068634,\n",
       "   0.5535196661949158,\n",
       "   0.05518198758363724,\n",
       "   0.037947289645671844,\n",
       "   0.030034935101866722,\n",
       "   0.040441691875457764,\n",
       "   0.01738293468952179,\n",
       "   0.013224351219832897],\n",
       "  [0.0034482465125620365,\n",
       "   0.0038895425386726856,\n",
       "   0.005236065946519375,\n",
       "   0.013813172467052937,\n",
       "   0.010430196300148964,\n",
       "   0.05077805370092392,\n",
       "   0.641498327255249,\n",
       "   0.01323755457997322,\n",
       "   0.16897264122962952,\n",
       "   0.08184266090393066,\n",
       "   0.0035076595377177,\n",
       "   0.0033459344413131475],\n",
       "  [0.011839332059025764,\n",
       "   0.009265673346817493,\n",
       "   0.020825976505875587,\n",
       "   0.4289425313472748,\n",
       "   0.027788354083895683,\n",
       "   0.02105570212006569,\n",
       "   0.014683406800031662,\n",
       "   0.3304857909679413,\n",
       "   0.05788344889879227,\n",
       "   0.0527602918446064,\n",
       "   0.017025265842676163,\n",
       "   0.00744414608925581],\n",
       "  [0.027381373569369316,\n",
       "   0.024950889870524406,\n",
       "   0.05205394700169563,\n",
       "   0.4092806279659271,\n",
       "   0.08168795704841614,\n",
       "   0.0769209936261177,\n",
       "   0.02724587544798851,\n",
       "   0.16947263479232788,\n",
       "   0.047082576900720596,\n",
       "   0.05163945257663727,\n",
       "   0.02144119143486023,\n",
       "   0.010842563584446907],\n",
       "  [0.0038758176378905773,\n",
       "   0.003917417023330927,\n",
       "   0.005418309476226568,\n",
       "   0.045663487166166306,\n",
       "   0.007978193461894989,\n",
       "   0.009805173613131046,\n",
       "   0.012987712398171425,\n",
       "   0.11252029985189438,\n",
       "   0.21147406101226807,\n",
       "   0.5791746377944946,\n",
       "   0.004015128128230572,\n",
       "   0.003169688628986478],\n",
       "  [0.07595512270927429,\n",
       "   0.06700103729963303,\n",
       "   0.08602173626422882,\n",
       "   0.13264940679073334,\n",
       "   0.0768849328160286,\n",
       "   0.08079946041107178,\n",
       "   0.07029906660318375,\n",
       "   0.09581216424703598,\n",
       "   0.08089634776115417,\n",
       "   0.074249267578125,\n",
       "   0.0905224159359932,\n",
       "   0.06890902668237686],\n",
       "  [0.08349008858203888,\n",
       "   0.08230611681938171,\n",
       "   0.08249524235725403,\n",
       "   0.0853073000907898,\n",
       "   0.07970943301916122,\n",
       "   0.0837709978222847,\n",
       "   0.08443307131528854,\n",
       "   0.08118802309036255,\n",
       "   0.08668394386768341,\n",
       "   0.0874016061425209,\n",
       "   0.08157685399055481,\n",
       "   0.08163739740848541]],\n",
       " 'p2h_attention': [[0.5873391032218933,\n",
       "   0.037481389939785004,\n",
       "   0.036728233098983765,\n",
       "   0.01829739846289158,\n",
       "   0.028163855895400047,\n",
       "   0.031889576464891434,\n",
       "   0.026729749515652657,\n",
       "   0.027614938095211983,\n",
       "   0.03713584318757057,\n",
       "   0.02042875997722149,\n",
       "   0.034058839082717896,\n",
       "   0.05014196038246155,\n",
       "   0.02343931421637535,\n",
       "   0.0210875291377306,\n",
       "   0.019463635981082916],\n",
       "  [2.508237594156526e-05,\n",
       "   0.999733567237854,\n",
       "   2.2132628146209754e-05,\n",
       "   1.4946935152693186e-05,\n",
       "   1.4804178135818802e-05,\n",
       "   1.5491494195885025e-05,\n",
       "   2.0095949366805144e-05,\n",
       "   2.0802033759537153e-05,\n",
       "   2.2308173356577754e-05,\n",
       "   1.6277719623758458e-05,\n",
       "   1.8829146938514896e-05,\n",
       "   3.227626802981831e-05,\n",
       "   1.6735255485400558e-05,\n",
       "   1.3140176633896772e-05,\n",
       "   1.3554149518313352e-05],\n",
       "  [0.05465352162718773,\n",
       "   0.04855705797672272,\n",
       "   0.056016504764556885,\n",
       "   0.07405351102352142,\n",
       "   0.07999254763126373,\n",
       "   0.0822991207242012,\n",
       "   0.04526546597480774,\n",
       "   0.10283025354146957,\n",
       "   0.1021103709936142,\n",
       "   0.04191756993532181,\n",
       "   0.08095712214708328,\n",
       "   0.12880918383598328,\n",
       "   0.044278454035520554,\n",
       "   0.0322718508541584,\n",
       "   0.025987526401877403],\n",
       "  [0.0009891841327771544,\n",
       "   0.0009495936101302505,\n",
       "   0.001004332909360528,\n",
       "   0.879708468914032,\n",
       "   0.010020802728831768,\n",
       "   0.010429514572024345,\n",
       "   0.02391846291720867,\n",
       "   0.012782299891114235,\n",
       "   0.0028063070494681597,\n",
       "   0.0019584111869335175,\n",
       "   0.02953033149242401,\n",
       "   0.0179363414645195,\n",
       "   0.006608717143535614,\n",
       "   0.00088133366080001,\n",
       "   0.00047592856572009623],\n",
       "  [0.021002642810344696,\n",
       "   0.01555121410638094,\n",
       "   0.023608563467860222,\n",
       "   0.10885298252105713,\n",
       "   0.11919141560792923,\n",
       "   0.08568055927753448,\n",
       "   0.04793670400977135,\n",
       "   0.22462216019630432,\n",
       "   0.07627135515213013,\n",
       "   0.04514884203672409,\n",
       "   0.0584084689617157,\n",
       "   0.10929856449365616,\n",
       "   0.035253044217824936,\n",
       "   0.01559625193476677,\n",
       "   0.013577163219451904],\n",
       "  [0.007228714879602194,\n",
       "   0.0060742199420928955,\n",
       "   0.011056041345000267,\n",
       "   0.00900836382061243,\n",
       "   0.07774664461612701,\n",
       "   0.21826252341270447,\n",
       "   0.035505931824445724,\n",
       "   0.23809383809566498,\n",
       "   0.2266567349433899,\n",
       "   0.08492054045200348,\n",
       "   0.01709878444671631,\n",
       "   0.0397634282708168,\n",
       "   0.0167390089482069,\n",
       "   0.0063324240036308765,\n",
       "   0.005512842908501625],\n",
       "  [0.0035782784689217806,\n",
       "   0.005359055008739233,\n",
       "   0.004149807151407003,\n",
       "   0.0055876621045172215,\n",
       "   0.008408895693719387,\n",
       "   0.012961878441274166,\n",
       "   0.17214486002922058,\n",
       "   0.027425561100244522,\n",
       "   0.014880097471177578,\n",
       "   0.7064885497093201,\n",
       "   0.007852272130548954,\n",
       "   0.009274972602725029,\n",
       "   0.014600914902985096,\n",
       "   0.003628138452768326,\n",
       "   0.0036590411327779293],\n",
       "  [0.007103194482624531,\n",
       "   0.007891187444329262,\n",
       "   0.008725148625671864,\n",
       "   0.054802581667900085,\n",
       "   0.04579288512468338,\n",
       "   0.03368080407381058,\n",
       "   0.02679453045129776,\n",
       "   0.04378972575068474,\n",
       "   0.0200247373431921,\n",
       "   0.02852955460548401,\n",
       "   0.34585878252983093,\n",
       "   0.11289872974157333,\n",
       "   0.24754595756530762,\n",
       "   0.009676819667220116,\n",
       "   0.006885323207825422],\n",
       "  [0.0065566846169531345,\n",
       "   0.007946044206619263,\n",
       "   0.0075373295694589615,\n",
       "   0.007862917147576809,\n",
       "   0.015765480697155,\n",
       "   0.03393528610467911,\n",
       "   0.02739201858639717,\n",
       "   0.011213157325983047,\n",
       "   0.014669311232864857,\n",
       "   0.33705487847328186,\n",
       "   0.05606571584939957,\n",
       "   0.029029978439211845,\n",
       "   0.43060502409935,\n",
       "   0.007562020793557167,\n",
       "   0.00680405693128705],\n",
       "  [0.00604318268597126,\n",
       "   0.00882710050791502,\n",
       "   0.00583982327952981,\n",
       "   0.018451131880283356,\n",
       "   0.012412630952894688,\n",
       "   0.015154428780078888,\n",
       "   0.022438034415245056,\n",
       "   0.011593696661293507,\n",
       "   0.01217340212315321,\n",
       "   0.10061518102884293,\n",
       "   0.0314955934882164,\n",
       "   0.019623102620244026,\n",
       "   0.7268270254135132,\n",
       "   0.0042776064947247505,\n",
       "   0.004228129982948303],\n",
       "  [0.05502630025148392,\n",
       "   0.048057641834020615,\n",
       "   0.05371672287583351,\n",
       "   0.06345729529857635,\n",
       "   0.07389234751462936,\n",
       "   0.07568860054016113,\n",
       "   0.052783865481615067,\n",
       "   0.06770184636116028,\n",
       "   0.06341211497783661,\n",
       "   0.05225980281829834,\n",
       "   0.1231694370508194,\n",
       "   0.09874189645051956,\n",
       "   0.06106431409716606,\n",
       "   0.06320204585790634,\n",
       "   0.04782582074403763],\n",
       "  [0.06751631200313568,\n",
       "   0.06632179766893387,\n",
       "   0.06473962217569351,\n",
       "   0.06741329282522202,\n",
       "   0.06538397073745728,\n",
       "   0.0662652850151062,\n",
       "   0.06368929892778397,\n",
       "   0.06451314687728882,\n",
       "   0.0660991445183754,\n",
       "   0.06830303370952606,\n",
       "   0.07378979027271271,\n",
       "   0.06841585785150528,\n",
       "   0.06605063378810883,\n",
       "   0.06592094898223877,\n",
       "   0.06557781249284744]],\n",
       " 'premise_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'wandering',\n",
       "  'along',\n",
       "  'the',\n",
       "  'shore',\n",
       "  'drinking',\n",
       "  'iced',\n",
       "  'tea',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'hypothesis_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'a',\n",
       "  'blanket',\n",
       "  'near',\n",
       "  'some',\n",
       "  'rocks',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'politics',\n",
       "  '.',\n",
       "  '@@NULL@@']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor.predict(\n",
    "  hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\",\n",
    "  premise=\"Two women are wandering along the shore drinking iced tea.\"\n",
    ")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction['premise_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral', 'e+c'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "#     entailment, contradiction, neutral = None\n",
    "    for hypothesis in doc.sents:\n",
    "        if (premise != hypothesis):\n",
    "            prediction = predictor.predict(hypothesis=hypothesis.text, premise=premise.text)\n",
    "            entailment, contradiction, neutral = prediction['label_probs']\n",
    "            results.loc[i] = [premise.text, hypothesis.text, entailment, contradiction, neutral, (entailment + (1 - contradiction)) / 2]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "      <th>e+c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.977611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.908410</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.904193</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.925338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.162772</td>\n",
       "      <td>0.917696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>0.911948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.781964</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.216260</td>\n",
       "      <td>0.890094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.755611</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.236309</td>\n",
       "      <td>0.873766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.767259</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.211775</td>\n",
       "      <td>0.873146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.739140</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.248626</td>\n",
       "      <td>0.863452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.271786</td>\n",
       "      <td>0.861288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.735134</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>0.854201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.727070</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.250334</td>\n",
       "      <td>0.852237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.717547</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.246891</td>\n",
       "      <td>0.840992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.678346</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.832578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.659936</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>0.804823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.604333</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.798962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.682849</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.228379</td>\n",
       "      <td>0.797039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.762202</td>\n",
       "      <td>0.184278</td>\n",
       "      <td>0.053520</td>\n",
       "      <td>0.788962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.429359</td>\n",
       "      <td>0.781854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.584167</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.776704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.566095</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.772994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.589310</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.772807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.571464</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.390039</td>\n",
       "      <td>0.766483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.763730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.538478</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.715452</td>\n",
       "      <td>0.238567</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>0.738443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>0.073129</td>\n",
       "      <td>0.378262</td>\n",
       "      <td>0.737740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.474978</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.461011</td>\n",
       "      <td>0.705483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>0.458307</td>\n",
       "      <td>0.689153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.523283</td>\n",
       "      <td>0.235219</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.644032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.381097</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>0.602704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.373584</td>\n",
       "      <td>0.176693</td>\n",
       "      <td>0.449723</td>\n",
       "      <td>0.598446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.386267</td>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.584250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>0.298593</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.515255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.269745</td>\n",
       "      <td>0.259522</td>\n",
       "      <td>0.470732</td>\n",
       "      <td>0.505112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.285518</td>\n",
       "      <td>0.384351</td>\n",
       "      <td>0.330131</td>\n",
       "      <td>0.450584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.234756</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>0.426192</td>\n",
       "      <td>0.447852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.352285</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.447454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.268840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.073138</td>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.265046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>0.526356</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.246635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.573239</td>\n",
       "      <td>0.388022</td>\n",
       "      <td>0.232750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.102812</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.195390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.344303</td>\n",
       "      <td>0.193247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.894391</td>\n",
       "      <td>0.104787</td>\n",
       "      <td>0.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.925812</td>\n",
       "      <td>0.071030</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "43   But I'm not totally sure what I should be doing?   \n",
       "44   But I'm not totally sure what I should be doing?   \n",
       "1                   I guess I am feeling kinda tired.   \n",
       "34   I feel stressed certainly, too much to do maybe?   \n",
       "8            I feel overwhelmed, a bit, maybe hungry.   \n",
       "9            I feel overwhelmed, a bit, maybe hungry.   \n",
       "32   I feel stressed certainly, too much to do maybe?   \n",
       "35   I feel stressed certainly, too much to do maybe?   \n",
       "27  I find myself wanting something, but I'm not s...   \n",
       "47   But I'm not totally sure what I should be doing?   \n",
       "40   But I'm not totally sure what I should be doing?   \n",
       "0                   I guess I am feeling kinda tired.   \n",
       "31  I find myself wanting something, but I'm not s...   \n",
       "41   But I'm not totally sure what I should be doing?   \n",
       "10           I feel overwhelmed, a bit, maybe hungry.   \n",
       "11           I feel overwhelmed, a bit, maybe hungry.   \n",
       "29  I find myself wanting something, but I'm not s...   \n",
       "3                   I guess I am feeling kinda tired.   \n",
       "39   I feel stressed certainly, too much to do maybe?   \n",
       "42   But I'm not totally sure what I should be doing?   \n",
       "33   I feel stressed certainly, too much to do maybe?   \n",
       "67     but a part of me wants to stay up, nonetheless   \n",
       "24  I find myself wanting something, but I'm not s...   \n",
       "36   I feel stressed certainly, too much to do maybe?   \n",
       "25  I find myself wanting something, but I'm not s...   \n",
       "70     but a part of me wants to stay up, nonetheless   \n",
       "68     but a part of me wants to stay up, nonetheless   \n",
       "26  I find myself wanting something, but I'm not s...   \n",
       "37   I feel stressed certainly, too much to do maybe?   \n",
       "2                   I guess I am feeling kinda tired.   \n",
       "28  I find myself wanting something, but I'm not s...   \n",
       "63       and it's really time for me to get to bed...   \n",
       "45   But I'm not totally sure what I should be doing?   \n",
       "12           I feel overwhelmed, a bit, maybe hungry.   \n",
       "55                               Now it's a lot later   \n",
       "15           I feel overwhelmed, a bit, maybe hungry.   \n",
       "4                   I guess I am feeling kinda tired.   \n",
       "66     but a part of me wants to stay up, nonetheless   \n",
       "59       and it's really time for me to get to bed...   \n",
       "51                               Now it's a lot later   \n",
       "30  I find myself wanting something, but I'm not s...   \n",
       "53                               Now it's a lot later   \n",
       "61       and it's really time for me to get to bed...   \n",
       "46   But I'm not totally sure what I should be doing?   \n",
       "21                                           I dunno.   \n",
       "60       and it's really time for me to get to bed...   \n",
       "50                               Now it's a lot later   \n",
       "58       and it's really time for me to get to bed...   \n",
       "22                                           I dunno.   \n",
       "6                   I guess I am feeling kinda tired.   \n",
       "\n",
       "                                           hypothesis  entailment  \\\n",
       "43  I find myself wanting something, but I'm not s...    0.956455   \n",
       "44   I feel stressed certainly, too much to do maybe?    0.908410   \n",
       "1                                            I dunno.    0.904193   \n",
       "34                                           I dunno.    0.912021   \n",
       "8                   I guess I am feeling kinda tired.    0.836310   \n",
       "9                                            I dunno.    0.867386   \n",
       "32                  I guess I am feeling kinda tired.    0.781964   \n",
       "35  I find myself wanting something, but I'm not s...    0.755611   \n",
       "27   I feel stressed certainly, too much to do maybe?    0.767259   \n",
       "47     but a part of me wants to stay up, nonetheless    0.768105   \n",
       "40                  I guess I am feeling kinda tired.    0.739140   \n",
       "0            I feel overwhelmed, a bit, maybe hungry.    0.725396   \n",
       "31     but a part of me wants to stay up, nonetheless    0.735134   \n",
       "41           I feel overwhelmed, a bit, maybe hungry.    0.727070   \n",
       "10  I find myself wanting something, but I'm not s...    0.717547   \n",
       "11   I feel stressed certainly, too much to do maybe?    0.678346   \n",
       "29                               Now it's a lot later    0.659936   \n",
       "3    I feel stressed certainly, too much to do maybe?    0.604333   \n",
       "39     but a part of me wants to stay up, nonetheless    0.682849   \n",
       "42                                           I dunno.    0.762202   \n",
       "33           I feel overwhelmed, a bit, maybe hungry.    0.567174   \n",
       "67  I find myself wanting something, but I'm not s...    0.584167   \n",
       "24                  I guess I am feeling kinda tired.    0.566095   \n",
       "36   But I'm not totally sure what I should be doing?    0.589310   \n",
       "25           I feel overwhelmed, a bit, maybe hungry.    0.571464   \n",
       "70                               Now it's a lot later    0.566257   \n",
       "68   I feel stressed certainly, too much to do maybe?    0.538478   \n",
       "26                                           I dunno.    0.715452   \n",
       "37                               Now it's a lot later    0.548609   \n",
       "2   I find myself wanting something, but I'm not s...    0.474978   \n",
       "28   But I'm not totally sure what I should be doing?    0.460000   \n",
       "63     but a part of me wants to stay up, nonetheless    0.523283   \n",
       "45                               Now it's a lot later    0.381097   \n",
       "12   But I'm not totally sure what I should be doing?    0.373584   \n",
       "55     but a part of me wants to stay up, nonetheless    0.386267   \n",
       "15     but a part of me wants to stay up, nonetheless    0.329103   \n",
       "4    But I'm not totally sure what I should be doing?    0.269745   \n",
       "66                                           I dunno.    0.285518   \n",
       "59  I find myself wanting something, but I'm not s...    0.234756   \n",
       "51  I find myself wanting something, but I'm not s...    0.247193   \n",
       "30       and it's really time for me to get to bed...    0.033333   \n",
       "53   But I'm not totally sure what I should be doing?    0.045921   \n",
       "61   But I'm not totally sure what I should be doing?    0.073138   \n",
       "46       and it's really time for me to get to bed...    0.019627   \n",
       "21                               Now it's a lot later    0.038739   \n",
       "60   I feel stressed certainly, too much to do maybe?    0.102812   \n",
       "50                                           I dunno.    0.021096   \n",
       "58                                           I dunno.    0.018045   \n",
       "22       and it's really time for me to get to bed...    0.000822   \n",
       "6        and it's really time for me to get to bed...    0.003158   \n",
       "\n",
       "    contradiction   neutral       e+c  \n",
       "43       0.001234  0.042311  0.977611  \n",
       "44       0.005447  0.086143  0.951481  \n",
       "1        0.053517  0.042290  0.925338  \n",
       "34       0.070635  0.017344  0.920693  \n",
       "8        0.000918  0.162772  0.917696  \n",
       "9        0.043489  0.089125  0.911948  \n",
       "32       0.001776  0.216260  0.890094  \n",
       "35       0.008080  0.236309  0.873766  \n",
       "27       0.020967  0.211775  0.873146  \n",
       "47       0.025418  0.206478  0.871344  \n",
       "40       0.012235  0.248626  0.863452  \n",
       "0        0.002819  0.271786  0.861288  \n",
       "31       0.026731  0.238136  0.854201  \n",
       "41       0.022596  0.250334  0.852237  \n",
       "10       0.035562  0.246891  0.840992  \n",
       "11       0.013190  0.308464  0.832578  \n",
       "29       0.050289  0.289775  0.804823  \n",
       "3        0.006409  0.389258  0.798962  \n",
       "39       0.088772  0.228379  0.797039  \n",
       "42       0.184278  0.053520  0.788962  \n",
       "33       0.003467  0.429359  0.781854  \n",
       "67       0.030758  0.385075  0.776704  \n",
       "24       0.020107  0.413798  0.772994  \n",
       "36       0.043696  0.366994  0.772807  \n",
       "25       0.038498  0.390039  0.766483  \n",
       "70       0.038796  0.394947  0.763730  \n",
       "68       0.052767  0.408755  0.742856  \n",
       "26       0.238567  0.045980  0.738443  \n",
       "37       0.073129  0.378262  0.737740  \n",
       "2        0.064011  0.461011  0.705483  \n",
       "28       0.081693  0.458307  0.689153  \n",
       "63       0.235219  0.241498  0.644032  \n",
       "45       0.175689  0.443214  0.602704  \n",
       "12       0.176693  0.449723  0.598446  \n",
       "55       0.217766  0.395967  0.584250  \n",
       "15       0.298593  0.372304  0.515255  \n",
       "4        0.259522  0.470732  0.505112  \n",
       "66       0.384351  0.330131  0.450584  \n",
       "59       0.339052  0.426192  0.447852  \n",
       "51       0.352285  0.400522  0.447454  \n",
       "30       0.486933  0.479734  0.273200  \n",
       "53       0.508242  0.445838  0.268840  \n",
       "61       0.543047  0.383816  0.265046  \n",
       "46       0.526356  0.454018  0.246635  \n",
       "21       0.573239  0.388022  0.232750  \n",
       "60       0.712032  0.185157  0.195390  \n",
       "50       0.634602  0.344303  0.193247  \n",
       "58       0.898486  0.083470  0.059780  \n",
       "22       0.894391  0.104787  0.053215  \n",
       "6        0.925812  0.071030  0.038673  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='e+c', ascending=False).loc[results['neutral'] < .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'I feel stressed'\n",
    "\n",
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "    prediction = predictor.predict(hypothesis=hypothesis, premise=premise.text)\n",
    "    entailment, contradiction, neutral = prediction['label_probs']\n",
    "    results.loc[i] = [premise.text, hypothesis, entailment, contradiction, neutral]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.985132</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.014467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.060882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.933847</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.063966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.833155</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.162525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.769592</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.189401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.493208</td>\n",
       "      <td>0.287141</td>\n",
       "      <td>0.219651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.115519</td>\n",
       "      <td>0.591396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.537717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.081763</td>\n",
       "      <td>0.259905</td>\n",
       "      <td>0.658333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise       hypothesis  \\\n",
       "4   I feel stressed certainly, too much to do maybe?  I feel stressed   \n",
       "0                  I guess I am feeling kinda tired.  I feel stressed   \n",
       "1           I feel overwhelmed, a bit, maybe hungry.  I feel stressed   \n",
       "3  I find myself wanting something, but I'm not s...  I feel stressed   \n",
       "5   But I'm not totally sure what I should be doing?  I feel stressed   \n",
       "2                                           I dunno.  I feel stressed   \n",
       "8     but a part of me wants to stay up, nonetheless  I feel stressed   \n",
       "7       and it's really time for me to get to bed...  I feel stressed   \n",
       "6                               Now it's a lot later  I feel stressed   \n",
       "\n",
       "   entailment  contradiction   neutral  \n",
       "4    0.985132       0.000401  0.014467  \n",
       "0    0.936851       0.002266  0.060882  \n",
       "1    0.933847       0.002187  0.063966  \n",
       "3    0.833155       0.004319  0.162525  \n",
       "5    0.769592       0.041008  0.189401  \n",
       "2    0.493208       0.287141  0.219651  \n",
       "8    0.293085       0.115519  0.591396  \n",
       "7    0.109000       0.353283  0.537717  \n",
       "6    0.081763       0.259905  0.658333  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='entailment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(shape):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    nlp.add_pipe(KerasSimilarityShim.load(nlp.path / 'similarity', nlp, shape[0]))\n",
    "\n",
    "    doc1 = nlp(u'The king of France is bald.')\n",
    "    doc2 = nlp(u'France has no king.')\n",
    "\n",
    "    print(\"Sentence 1:\", doc1)\n",
    "    print(\"Sentence 2:\", doc2)\n",
    "\n",
    "    entailment_type, confidence = doc1.similarity(doc2)\n",
    "    print(\"Entailment type:\", entailment_type, \"(Confidence:\", confidence, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    "vectorizer = Vectorizer(\n",
    "    tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "    min_df=3, max_df=0.95, max_n_terms=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=20)\n",
    "model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.17393909018065556),\n",
       " ('overwhelmed', 0.1291297332498814),\n",
       " ('time', 0.12848449534695075),\n",
       " ('bit', 0.12152651186559832),\n",
       " ('lot', 0.12055447304217964),\n",
       " ('hungry', 0.11759733293982846),\n",
       " ('tired', 0.07154722366605952),\n",
       " ('bed', 0.0712721534037857),\n",
       " ('stressed', 0.06594898630506062)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.33850825516726607),\n",
       " ('stressed', 0.1517767729454664),\n",
       " ('bed', 0.1484936700017998),\n",
       " ('time', 0.0968496495027204),\n",
       " ('lot', 0.07202910699164278),\n",
       " ('hungry', 0.07058674833731196),\n",
       " ('bit', 0.051420957354160426),\n",
       " ('overwhelmed', 0.03964374936267677),\n",
       " ('tired', 0.030691090336955648)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.sgrank(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.lexicon_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:16:22 - INFO - textacy.lexicon_methods -   Downloaded DepecheMood (4MB) from https://github.com/marcoguerini/DepecheMood/releases/download/v1.0/DepecheMood_V1.0.zip and wrote it to data\n"
     ]
    }
   ],
   "source": [
    "textacy.lexicon_methods.download_depechemood(data_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'AFRAID': 0.1103335419756097,\n",
       "             'AMUSED': 0.14808456209756102,\n",
       "             'ANGRY': 0.10694153351219511,\n",
       "             'ANNOYED': 0.12443051617073168,\n",
       "             'DONT_CARE': 0.13096818899999998,\n",
       "             'HAPPY': 0.11531756726829266,\n",
       "             'INSPIRED': 0.14843126431707318,\n",
       "             'SAD': 0.11549282553658531})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.lexicon_methods.emotional_valence(words=[word for word in doc], dm_data_dir='data/DepecheMood_V1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   loading archive file data/event2mind.tar.gz\n",
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   extracting archive file data/event2mind.tar.gz to temp dir /tmp/tmp0dlhchct\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/06/2018 17:58:28 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp0dlhchct/vocabulary.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Model'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens', 'type': 'event2mind'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Event2Mind'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.type = basic\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.type = embedding\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.vocab_namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.embedding_dim = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.projection_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.trainable = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.padding_index = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.max_norm = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.sparse = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.embedding_dropout = 0.2\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.type = gru\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.hidden_size = 50\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.input_size = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.num_layers = 1\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.max_decoding_steps = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.beam_size = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_names = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_embedding_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}, 'type': 'event2mind'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.event2mind.Event2MindDatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'type': 'spacy'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.type = spacy\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.language = en_core_web_sm\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.pos_tags = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.parse = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.ner = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'source_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_add_start_token = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.dummy_instances_for_vocab_generation = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xintent_top_k_predictions': [[4, 3, 3, 3],\n",
       "  [684, 3, 3, 3],\n",
       "  [323, 3, 3, 3],\n",
       "  [282, 3, 3, 3],\n",
       "  [255, 3, 3, 3],\n",
       "  [153, 229, 3, 3],\n",
       "  [13, 175, 3, 3],\n",
       "  [44, 251, 3, 3],\n",
       "  [13, 267, 3, 3],\n",
       "  [211, 32, 80, 3]],\n",
       " 'xintent_top_k_log_probabilities': [-1.128340721130371,\n",
       "  -4.298313617706299,\n",
       "  -4.499514579772949,\n",
       "  -4.616389751434326,\n",
       "  -4.652942657470703,\n",
       "  -5.256786823272705,\n",
       "  -5.543419361114502,\n",
       "  -6.178347587585449,\n",
       "  -6.431229114532471,\n",
       "  -9.508707046508789],\n",
       " 'xreact_top_k_predictions': [[54, 3],\n",
       "  [70, 3],\n",
       "  [53, 3],\n",
       "  [109, 3],\n",
       "  [5, 3],\n",
       "  [73, 3],\n",
       "  [11, 3],\n",
       "  [92, 3],\n",
       "  [25, 3],\n",
       "  [63, 3]],\n",
       " 'xreact_top_k_log_probabilities': [-3.147449016571045,\n",
       "  -3.159241199493408,\n",
       "  -3.1768059730529785,\n",
       "  -3.2743079662323,\n",
       "  -3.3373990058898926,\n",
       "  -3.516559362411499,\n",
       "  -3.596619129180908,\n",
       "  -3.8112082481384277,\n",
       "  -3.8679120540618896,\n",
       "  -3.9374184608459473],\n",
       " 'oreact_top_k_predictions': [[4, 3],\n",
       "  [63, 3],\n",
       "  [36, 3],\n",
       "  [89, 3],\n",
       "  [83, 3],\n",
       "  [91, 3],\n",
       "  [138, 3],\n",
       "  [53, 3],\n",
       "  [92, 3],\n",
       "  [5, 3]],\n",
       " 'oreact_top_k_log_probabilities': [-1.067413330078125,\n",
       "  -2.62384033203125,\n",
       "  -3.9126994609832764,\n",
       "  -3.979201316833496,\n",
       "  -4.0716962814331055,\n",
       "  -4.296855926513672,\n",
       "  -4.327937602996826,\n",
       "  -4.378903388977051,\n",
       "  -4.453375339508057,\n",
       "  -4.506431579589844],\n",
       " 'xintent_top_k_predicted_tokens': [['none'],\n",
       "  ['annoying'],\n",
       "  ['noticed'],\n",
       "  ['communicate'],\n",
       "  ['heard'],\n",
       "  ['express', 'anger'],\n",
       "  ['get', 'attention'],\n",
       "  ['show', 'affection'],\n",
       "  ['get', 'revenge'],\n",
       "  ['let', 'someone', 'know']],\n",
       " 'xreact_top_k_predicted_tokens': [['upset'],\n",
       "  ['worried'],\n",
       "  ['nervous'],\n",
       "  ['curious'],\n",
       "  ['happy'],\n",
       "  ['scared'],\n",
       "  ['satisfied'],\n",
       "  ['anxious'],\n",
       "  ['relieved'],\n",
       "  ['annoyed']],\n",
       " 'oreact_top_k_predicted_tokens': [['none'],\n",
       "  ['annoyed'],\n",
       "  ['angry'],\n",
       "  ['informed'],\n",
       "  ['surprised'],\n",
       "  ['interested'],\n",
       "  ['frustrated'],\n",
       "  ['nervous'],\n",
       "  ['anxious'],\n",
       "  ['happy']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from event2mind_hack import load_event2mind_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_event2mind_archive('data/event2mind.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)\n",
    "predictor.predict(\n",
    "  source=\"PersonX drops a hint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.128341</td>\n",
       "      <td>0.323570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoying]</td>\n",
       "      <td>-4.298314</td>\n",
       "      <td>0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noticed]</td>\n",
       "      <td>-4.499515</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[communicate]</td>\n",
       "      <td>-4.616390</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[heard]</td>\n",
       "      <td>-4.652943</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[express, anger]</td>\n",
       "      <td>-5.256787</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[get, attention]</td>\n",
       "      <td>-5.543419</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, affection]</td>\n",
       "      <td>-6.178348</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[get, revenge]</td>\n",
       "      <td>-6.431229</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[let, someone, know]</td>\n",
       "      <td>-9.508707</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tokens     p_log         p\n",
       "0                [none] -1.128341  0.323570\n",
       "1            [annoying] -4.298314  0.013591\n",
       "2             [noticed] -4.499515  0.011114\n",
       "3         [communicate] -4.616390  0.009888\n",
       "4               [heard] -4.652943  0.009534\n",
       "5      [express, anger] -5.256787  0.005212\n",
       "6      [get, attention] -5.543419  0.003913\n",
       "7     [show, affection] -6.178348  0.002074\n",
       "8        [get, revenge] -6.431229  0.001610\n",
       "9  [let, someone, know] -9.508707  0.000074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xintent = pd.DataFrame({\n",
    "    'tokens': prediction['xintent_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xintent_top_k_log_probabilities']\n",
    "})\n",
    "xintent['p'] = xintent['p_log'].apply(math.exp)\n",
    "xintent.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upset]</td>\n",
       "      <td>-3.147449</td>\n",
       "      <td>0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worried]</td>\n",
       "      <td>-3.159241</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-3.176806</td>\n",
       "      <td>0.041719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[curious]</td>\n",
       "      <td>-3.274308</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-3.337399</td>\n",
       "      <td>0.035529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[scared]</td>\n",
       "      <td>-3.516559</td>\n",
       "      <td>0.029701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[satisfied]</td>\n",
       "      <td>-3.596619</td>\n",
       "      <td>0.027416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-3.811208</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[relieved]</td>\n",
       "      <td>-3.867912</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-3.937418</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens     p_log         p\n",
       "0      [upset] -3.147449  0.042962\n",
       "1    [worried] -3.159241  0.042458\n",
       "2    [nervous] -3.176806  0.041719\n",
       "3    [curious] -3.274308  0.037843\n",
       "4      [happy] -3.337399  0.035529\n",
       "5     [scared] -3.516559  0.029701\n",
       "6  [satisfied] -3.596619  0.027416\n",
       "7    [anxious] -3.811208  0.022121\n",
       "8   [relieved] -3.867912  0.020902\n",
       "9    [annoyed] -3.937418  0.019498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreact = pd.DataFrame({\n",
    "    'tokens': prediction['xreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xreact_top_k_log_probabilities']\n",
    "})\n",
    "xreact['p'] = xreact['p_log'].apply(math.exp)\n",
    "xreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.067413</td>\n",
       "      <td>0.343897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-2.623840</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[angry]</td>\n",
       "      <td>-3.912699</td>\n",
       "      <td>0.019986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[informed]</td>\n",
       "      <td>-3.979201</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surprised]</td>\n",
       "      <td>-4.071696</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[interested]</td>\n",
       "      <td>-4.296856</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[frustrated]</td>\n",
       "      <td>-4.327938</td>\n",
       "      <td>0.013195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-4.378903</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-4.453375</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-4.506432</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens     p_log         p\n",
       "0        [none] -1.067413  0.343897\n",
       "1     [annoyed] -2.623840  0.072524\n",
       "2       [angry] -3.912699  0.019986\n",
       "3    [informed] -3.979201  0.018701\n",
       "4   [surprised] -4.071696  0.017048\n",
       "5  [interested] -4.296856  0.013611\n",
       "6  [frustrated] -4.327938  0.013195\n",
       "7     [nervous] -4.378903  0.012539\n",
       "8     [anxious] -4.453375  0.011639\n",
       "9       [happy] -4.506432  0.011038"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oreact = pd.DataFrame({\n",
    "    'tokens': prediction['oreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['oreact_top_k_log_probabilities']\n",
    "})\n",
    "oreact['p'] = oreact['p_log'].apply(math.exp)\n",
    "oreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
