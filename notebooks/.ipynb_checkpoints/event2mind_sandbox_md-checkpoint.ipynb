{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"So I have had a good day today. I found out we got the other half of our funding for my travel grant, which paid for my friend to come with me. So that’s good, she and I will both get some money back. I took my dogs to the pet store so my girl dog could get a new collar, but she wanted to beat everyone up. This is an ongoing issue with her. She’s so little and cute too but damn she acts like she’s gonna go for the jugular with everyone she doesn’t know! She did end up with a cute new collar tho, it has pineapples on it. I went to the dentist and she’s happy with my Invisalign progress. We have three more trays and then she does an impression to make sure my teeth are where they need to be before they get the rest of the trays. YAY! And I don’t have to make another payment until closer to the end of my treatment. I had some work emails with the festival, and Jessie was bringing up some important points, and one of our potential artists was too expensive to work with, so Mutual Friend was asking for names for some other people we could work with. So I suggested like, three artists, and Jessie actually liked the idea of one of them doing it. Which is nice. I notice she is very encouraging at whatever I contribute to our collective. It’s sweet. I kind of know this is like, the only link we have with each other right now besides social media, so it seems like she’s trying to make sure I know she still wants me to be involved and doesn’t have bad feelings for me. And there was a short period when I was seriously thinking of leaving the collective and not working with this festival anymore. I was so sad, and felt so upset, and didn’t know what to do about Jessie. It felt really close to me throwing in the towel. But I hung on through the festival and it doesn’t seem so bad from this viewpoint now with more time that has passed. And we have been gentle, if reserved, with each other. I mean her last personal email to me however many weeks ago wasn’t very nice. But it seems like we’ve been able to put it aside for work reasons. I dunno. I still feel like if anything was gonna get mended between us, she would need to make the first moves on that. I really don’t want to try reaching out and get rejected even as a friend again. I miss her though. And sometimes I think she misses me. But I don’t want to approach her assuming we both miss each other and have her turn it on me again and make out like all these things are all in my head. I don’t know about that butch I went on a date with last night. I feel more of a friend vibe from her, than a romantic one. I can’t help it, I am just not attracted to butches. And I don’t know how to flirt with them. And I don’t think of them in a sexy way. But I WOULD like another butch buddy. I mean yeah maybe Femmes do play games, or maybe I just chased all the wrong Femmes. Maybe I’ll just leave this and not think about it much until I get back to town in January.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Subject Verb Object Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I, have had, day)\n",
      "(we, got, half)\n",
      "(she, get, money)\n",
      "(I, get, money)\n",
      "(I, took, dogs)\n",
      "(girl dog, could get, collar)\n",
      "(she, wanted, to beat)\n",
      "(This, is, issue)\n",
      "(she, ’s gon, na go)\n",
      "(it, has, pineapples)\n",
      "(We, have, trays)\n",
      "(she, does, impression)\n",
      "(they, need, to be)\n",
      "(they, get, rest)\n",
      "(I, don’t have, to make)\n",
      "(I, had, work emails)\n",
      "(Jessie, was bringing, points)\n",
      "(artists, liked, idea)\n",
      "(Jessie, liked, idea)\n",
      "(one, doing, it)\n",
      "(this, is, link)\n",
      "(she, ’s trying, to make)\n",
      "(there, was, period)\n",
      "(It, felt, throwing)\n",
      "(it, doesn’t seem, bad)\n",
      "(anything, was gon, na get mended)\n",
      "(she, would need, to make)\n",
      "(I, don’t want, to try)\n",
      "(I, don’t want, get rejected)\n",
      "(I, miss, her)\n",
      "(she, misses, me)\n",
      "(I, don’t want, to approach)\n",
      "(we, miss, other)\n",
      "(her, turn, it)\n",
      "(I, feel, more)\n",
      "(I, can’t help, it)\n",
      "(I, don’t know, to flirt)\n",
      "(I, WOULD like, butch buddy)\n",
      "(Femmes, do play, games)\n",
      "(I, chased, Femmes)\n",
      "(I, leave, this)\n"
     ]
    }
   ],
   "source": [
    "svo_triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "for triple in svo_triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a good day DATE\n",
      "today DATE\n",
      "the other half CARDINAL\n",
      "three CARDINAL\n",
      "Jessie PERSON\n",
      "one CARDINAL\n",
      "Mutual Friend PERSON\n",
      "three CARDINAL\n",
      "Jessie PERSON\n",
      "one CARDINAL\n",
      "Jessie PERSON\n",
      "many weeks ago DATE\n",
      "first ORDINAL\n",
      "last night TIME\n",
      "Femmes PERSON\n",
      "January DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am feeling --> kinda tired\n",
      "I feel --> overwhelmed, a bit, maybe hungry\n",
      "I feel --> stressed certainly, too much to do maybe\n"
     ]
    }
   ],
   "source": [
    "# returns (entity, cue, fragment)\n",
    "statements = textacy.extract.semistructured_statements(doc, 'I', cue='feel')\n",
    "\n",
    "for entity, cue, fragment in statements:\n",
    "    print(entity, cue, '-->', fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: I guess I am feeling kinda tired. \n",
      "verbs: [guess, feeling]\n",
      "I guess (I, guess, I am feeling kinda tired)\n",
      "I feeling (I, am feeling, kinda tired)\n",
      "I feeling (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feeling (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: I feel overwhelmed, a bit, maybe hungry. \n",
      "verbs: [feel]\n",
      "I feel (I, am feeling, kinda tired)\n",
      "I feel (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feel (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: I dunno. \n",
      "verbs: [dunno]\n",
      "\n",
      "\n",
      "sent: I find myself wanting something, but I'm not sure what it is. \n",
      "verbs: [find, wanting, 'm, is]\n",
      "I find (I, find, myself wanting something, but I'm not sure what it is)\n",
      "I 'm (I, 'm, not sure what it is)\n",
      "I 'm (I, 'm, not totally sure what I should be doing)\n",
      "\n",
      "\n",
      "sent: I feel stressed certainly, too much to do maybe? \n",
      "verbs: [feel, do]\n",
      "I feel (I, am feeling, kinda tired)\n",
      "I feel (I, feel, overwhelmed, a bit, maybe hungry)\n",
      "I feel (I, feel, stressed certainly, too much to do maybe)\n",
      "\n",
      "\n",
      "sent: But I'm not totally sure what I should be doing? \n",
      "verbs: ['m, doing]\n",
      "I 'm (I, 'm, not sure what it is)\n",
      "I 'm (I, 'm, not totally sure what I should be doing)\n",
      "\n",
      "\n",
      "(I, feel, stressed certainly, too much to do maybe)\n",
      "(I, 'm, not totally sure what I should be doing)\n",
      "(I, am feeling, kinda tired)\n",
      "(I, 'm, not sure what it is)\n",
      "(I, feel, overwhelmed, a bit, maybe hungry)\n",
      "(I, find, myself wanting something, but I'm not sure what it is)\n",
      "(I, guess, I am feeling kinda tired)\n"
     ]
    }
   ],
   "source": [
    "# get cues\n",
    "all_statements = []\n",
    "for sent in doc.sents:\n",
    "    verbs = textacy.spacier.utils.get_main_verbs_of_sent(sent)\n",
    "    print('sent:', sent, '\\nverbs:', verbs)\n",
    "    for verb in verbs:\n",
    "        objects = textacy.spacier.utils.get_objects_of_verb(verb)\n",
    "        subjects = textacy.spacier.utils.get_subjects_of_verb(verb)\n",
    "        for subject in subjects:\n",
    "            statements = textacy.extract.semistructured_statements(doc, subject.text, verb.lemma_)\n",
    "            for statement in statements:\n",
    "                print(subject, verb, statement)\n",
    "                all_statements += [statement]\n",
    "    \n",
    "    print('\\n')\n",
    "for statement in set(all_statements):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:56:34 - INFO - allennlp.models.archival -   loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz from cache at /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8\n",
      "12/05/2018 21:56:34 - INFO - allennlp.models.archival -   extracting archive file /home/russell/.allennlp/cache/1dbdfb3ce5af46c5b83353727b579a5596d45a121d59199f1c838928a87e3796.21e6e14db76ce734b669577cc3046333c6bc853767246356b4a8b2c6a85249a8 to temp dir /tmp/tmpdvw280lq\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   type = default\n",
      "12/05/2018 21:56:42 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpdvw280lq/vocabulary.\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'type': 'decomposable_attention', 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'similarity_function': {'type': 'dot_product'}, 'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'do_layer_norm': False, 'type': 'elmo_token_embedder', 'dropout': 0.2, 'options_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.type = decomposable_attention\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.decomposable_attention.DecomposableAttention'> from params {'initializer': [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]], 'aggregate_feedforward': {'activations': ['relu', 'linear'], 'dropout': [0.2, 0], 'hidden_dims': [200, 3], 'input_dim': 400, 'num_layers': 2}, 'similarity_function': {'type': 'dot_product'}, 'compare_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 2048, 'num_layers': 2}, 'attend_feedforward': {'activations': 'relu', 'dropout': 0.2, 'hidden_dims': 200, 'input_dim': 1024, 'num_layers': 2}, 'text_field_embedder': {'elmo': {'do_layer_norm': False, 'type': 'elmo_token_embedder', 'dropout': 0.2, 'options_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'do_layer_norm': False, 'type': 'elmo_token_embedder', 'dropout': 0.2, 'options_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'do_layer_norm': False, 'type': 'elmo_token_embedder', 'dropout': 0.2, 'options_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.options_file\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpdvw280lq/fta/model.text_field_embedder.elmo.weight_file\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0.2\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "12/05/2018 21:56:42 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "12/05/2018 21:56:42 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.attend_feedforward.input_dim = 1024\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.attend_feedforward.num_layers = 2\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.attend_feedforward.hidden_dims = 200\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.attend_feedforward.activations = relu\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.attend_feedforward.dropout = 0.2\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.similarity_functions.similarity_function.SimilarityFunction'> from params {'type': 'dot_product'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.similarity_function.type = dot_product\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.similarity_functions.dot_product.DotProductSimilarity'> from params {} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7f8405bc7748>}\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.similarity_function.scale_output = False\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.compare_feedforward.input_dim = 2048\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.compare_feedforward.num_layers = 2\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.compare_feedforward.hidden_dims = 200\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.compare_feedforward.activations = relu\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.compare_feedforward.dropout = 0.2\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.aggregate_feedforward.input_dim = 400\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.aggregate_feedforward.num_layers = 2\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.aggregate_feedforward.hidden_dims = [200, 3]\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.aggregate_feedforward.activations = ['relu', 'linear']\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.aggregate_feedforward.dropout = [0.2, 0]\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.initializer = [['.*linear_layers.*weight', {'type': 'xavier_normal'}], ['.*token_embedder_tokens\\\\._projection.*weight', {'type': 'xavier_normal'}]]\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   model.initializer.list.list.type = xavier_normal\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/05/2018 21:56:56 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _attend_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _attend_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _compare_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _compare_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _aggregate_feedforward._linear_layers.0.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Initializing _aggregate_feedforward._linear_layers.1.weight using .*linear_layers.*weight intitializer\n",
      "12/05/2018 21:56:56 - WARNING - allennlp.nn.initializers -   Did not use initialization regex that was passed: .*token_embedder_tokens\\._projection.*weight\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -      _aggregate_feedforward._linear_layers.0.bias\n",
      "12/05/2018 21:56:56 - INFO - allennlp.nn.initializers -      _aggregate_feedforward._linear_layers.1.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _attend_feedforward._module._linear_layers.0.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _attend_feedforward._module._linear_layers.1.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _compare_feedforward._module._linear_layers.0.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _compare_feedforward._module._linear_layers.1.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "12/05/2018 21:56:57 - INFO - allennlp.nn.initializers -      _text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'elmo': {'type': 'elmo_characters'}}, 'tokenizer': {'end_tokens': ['@@NULL@@']}, 'type': 'snli'} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.type = snli\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.snli.SnliReader'> from params {'token_indexers': {'elmo': {'type': 'elmo_characters'}}, 'tokenizer': {'end_tokens': ['@@NULL@@']}} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'end_tokens': ['@@NULL@@']} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.tokenizer.type = word\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'end_tokens': ['@@NULL@@']} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.tokenizer.start_tokens = None\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.tokenizer.end_tokens = ['@@NULL@@']\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'type': 'elmo_characters'} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer'> from params {} and extras {}\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "12/05/2018 21:56:57 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_logits': [-3.391864776611328, 4.570619106292725, 0.9505535364151001],\n",
       " 'label_probs': [0.00033908773912116885,\n",
       "  0.9735872745513916,\n",
       "  0.02607356198132038],\n",
       " 'h2p_attention': [[0.6615484952926636,\n",
       "   0.03999358043074608,\n",
       "   0.04555582255125046,\n",
       "   0.046556826680898666,\n",
       "   0.032376978546381,\n",
       "   0.02884303592145443,\n",
       "   0.021681087091565132,\n",
       "   0.02199293114244938,\n",
       "   0.021933946758508682,\n",
       "   0.03280186280608177,\n",
       "   0.02464543841779232,\n",
       "   0.0220700204372406],\n",
       "  [2.6478128347662278e-05,\n",
       "   0.9997804164886475,\n",
       "   2.5384990294696763e-05,\n",
       "   2.803125425998587e-05,\n",
       "   1.5035763681225944e-05,\n",
       "   1.5200890629785135e-05,\n",
       "   2.0365438103908673e-05,\n",
       "   1.5323941624956205e-05,\n",
       "   1.667179458308965e-05,\n",
       "   3.005035614478402e-05,\n",
       "   1.3499801752914209e-05,\n",
       "   1.3597185898106545e-05],\n",
       "  [0.10203886777162552,\n",
       "   0.08704567700624466,\n",
       "   0.11516872048377991,\n",
       "   0.11659414321184158,\n",
       "   0.0897686704993248,\n",
       "   0.1088109016418457,\n",
       "   0.06201941892504692,\n",
       "   0.06663387268781662,\n",
       "   0.06219317018985748,\n",
       "   0.07818535715341568,\n",
       "   0.0593428835272789,\n",
       "   0.052198376506567],\n",
       "  [0.0004895933088846505,\n",
       "   0.0005661703762598336,\n",
       "   0.0014663741458207369,\n",
       "   0.9835996627807617,\n",
       "   0.003986356779932976,\n",
       "   0.0008538846741430461,\n",
       "   0.0008042860426940024,\n",
       "   0.004030915908515453,\n",
       "   0.000624869717285037,\n",
       "   0.002379189943894744,\n",
       "   0.0006751827313564718,\n",
       "   0.0005234954296611249],\n",
       "  [0.02180355414748192,\n",
       "   0.016224386170506477,\n",
       "   0.04582870751619339,\n",
       "   0.32416868209838867,\n",
       "   0.1262902319431305,\n",
       "   0.21321798861026764,\n",
       "   0.03501936420798302,\n",
       "   0.09745176136493683,\n",
       "   0.036249496042728424,\n",
       "   0.04630831629037857,\n",
       "   0.022747211158275604,\n",
       "   0.014690198004245758],\n",
       "  [0.01745988056063652,\n",
       "   0.012007005512714386,\n",
       "   0.03334576636552811,\n",
       "   0.23861074447631836,\n",
       "   0.06420435011386871,\n",
       "   0.42332977056503296,\n",
       "   0.03817636892199516,\n",
       "   0.050691068172454834,\n",
       "   0.05518278479576111,\n",
       "   0.03998453542590141,\n",
       "   0.016478458419442177,\n",
       "   0.010529308579862118],\n",
       "  [0.010657047852873802,\n",
       "   0.011342254467308521,\n",
       "   0.013355554081499577,\n",
       "   0.39848199486732483,\n",
       "   0.026157716289162636,\n",
       "   0.050147585570812225,\n",
       "   0.36920735239982605,\n",
       "   0.02936599962413311,\n",
       "   0.03243587911128998,\n",
       "   0.04311087727546692,\n",
       "   0.008368291892111301,\n",
       "   0.00736935855820775],\n",
       "  [0.012434241361916065,\n",
       "   0.013259582221508026,\n",
       "   0.03426486626267433,\n",
       "   0.24050143361091614,\n",
       "   0.13842594623565674,\n",
       "   0.37977859377861023,\n",
       "   0.06643011420965195,\n",
       "   0.05420059710741043,\n",
       "   0.014995560981333256,\n",
       "   0.025156904011964798,\n",
       "   0.01212186086922884,\n",
       "   0.008430331014096737],\n",
       "  [0.025600627064704895,\n",
       "   0.02177058905363083,\n",
       "   0.05209308862686157,\n",
       "   0.08083993941545486,\n",
       "   0.0719628632068634,\n",
       "   0.5535196661949158,\n",
       "   0.05518198758363724,\n",
       "   0.037947289645671844,\n",
       "   0.030034935101866722,\n",
       "   0.040441691875457764,\n",
       "   0.01738293468952179,\n",
       "   0.013224351219832897],\n",
       "  [0.0034482465125620365,\n",
       "   0.0038895425386726856,\n",
       "   0.005236065946519375,\n",
       "   0.013813172467052937,\n",
       "   0.010430196300148964,\n",
       "   0.05077805370092392,\n",
       "   0.641498327255249,\n",
       "   0.01323755457997322,\n",
       "   0.16897264122962952,\n",
       "   0.08184266090393066,\n",
       "   0.0035076595377177,\n",
       "   0.0033459344413131475],\n",
       "  [0.011839332059025764,\n",
       "   0.009265673346817493,\n",
       "   0.020825976505875587,\n",
       "   0.4289425313472748,\n",
       "   0.027788354083895683,\n",
       "   0.02105570212006569,\n",
       "   0.014683406800031662,\n",
       "   0.3304857909679413,\n",
       "   0.05788344889879227,\n",
       "   0.0527602918446064,\n",
       "   0.017025265842676163,\n",
       "   0.00744414608925581],\n",
       "  [0.027381373569369316,\n",
       "   0.024950889870524406,\n",
       "   0.05205394700169563,\n",
       "   0.4092806279659271,\n",
       "   0.08168795704841614,\n",
       "   0.0769209936261177,\n",
       "   0.02724587544798851,\n",
       "   0.16947263479232788,\n",
       "   0.047082576900720596,\n",
       "   0.05163945257663727,\n",
       "   0.02144119143486023,\n",
       "   0.010842563584446907],\n",
       "  [0.0038758176378905773,\n",
       "   0.003917417023330927,\n",
       "   0.005418309476226568,\n",
       "   0.045663487166166306,\n",
       "   0.007978193461894989,\n",
       "   0.009805173613131046,\n",
       "   0.012987712398171425,\n",
       "   0.11252029985189438,\n",
       "   0.21147406101226807,\n",
       "   0.5791746377944946,\n",
       "   0.004015128128230572,\n",
       "   0.003169688628986478],\n",
       "  [0.07595512270927429,\n",
       "   0.06700103729963303,\n",
       "   0.08602173626422882,\n",
       "   0.13264940679073334,\n",
       "   0.0768849328160286,\n",
       "   0.08079946041107178,\n",
       "   0.07029906660318375,\n",
       "   0.09581216424703598,\n",
       "   0.08089634776115417,\n",
       "   0.074249267578125,\n",
       "   0.0905224159359932,\n",
       "   0.06890902668237686],\n",
       "  [0.08349008858203888,\n",
       "   0.08230611681938171,\n",
       "   0.08249524235725403,\n",
       "   0.0853073000907898,\n",
       "   0.07970943301916122,\n",
       "   0.0837709978222847,\n",
       "   0.08443307131528854,\n",
       "   0.08118802309036255,\n",
       "   0.08668394386768341,\n",
       "   0.0874016061425209,\n",
       "   0.08157685399055481,\n",
       "   0.08163739740848541]],\n",
       " 'p2h_attention': [[0.5873391032218933,\n",
       "   0.037481389939785004,\n",
       "   0.036728233098983765,\n",
       "   0.01829739846289158,\n",
       "   0.028163855895400047,\n",
       "   0.031889576464891434,\n",
       "   0.026729749515652657,\n",
       "   0.027614938095211983,\n",
       "   0.03713584318757057,\n",
       "   0.02042875997722149,\n",
       "   0.034058839082717896,\n",
       "   0.05014196038246155,\n",
       "   0.02343931421637535,\n",
       "   0.0210875291377306,\n",
       "   0.019463635981082916],\n",
       "  [2.508237594156526e-05,\n",
       "   0.999733567237854,\n",
       "   2.2132628146209754e-05,\n",
       "   1.4946935152693186e-05,\n",
       "   1.4804178135818802e-05,\n",
       "   1.5491494195885025e-05,\n",
       "   2.0095949366805144e-05,\n",
       "   2.0802033759537153e-05,\n",
       "   2.2308173356577754e-05,\n",
       "   1.6277719623758458e-05,\n",
       "   1.8829146938514896e-05,\n",
       "   3.227626802981831e-05,\n",
       "   1.6735255485400558e-05,\n",
       "   1.3140176633896772e-05,\n",
       "   1.3554149518313352e-05],\n",
       "  [0.05465352162718773,\n",
       "   0.04855705797672272,\n",
       "   0.056016504764556885,\n",
       "   0.07405351102352142,\n",
       "   0.07999254763126373,\n",
       "   0.0822991207242012,\n",
       "   0.04526546597480774,\n",
       "   0.10283025354146957,\n",
       "   0.1021103709936142,\n",
       "   0.04191756993532181,\n",
       "   0.08095712214708328,\n",
       "   0.12880918383598328,\n",
       "   0.044278454035520554,\n",
       "   0.0322718508541584,\n",
       "   0.025987526401877403],\n",
       "  [0.0009891841327771544,\n",
       "   0.0009495936101302505,\n",
       "   0.001004332909360528,\n",
       "   0.879708468914032,\n",
       "   0.010020802728831768,\n",
       "   0.010429514572024345,\n",
       "   0.02391846291720867,\n",
       "   0.012782299891114235,\n",
       "   0.0028063070494681597,\n",
       "   0.0019584111869335175,\n",
       "   0.02953033149242401,\n",
       "   0.0179363414645195,\n",
       "   0.006608717143535614,\n",
       "   0.00088133366080001,\n",
       "   0.00047592856572009623],\n",
       "  [0.021002642810344696,\n",
       "   0.01555121410638094,\n",
       "   0.023608563467860222,\n",
       "   0.10885298252105713,\n",
       "   0.11919141560792923,\n",
       "   0.08568055927753448,\n",
       "   0.04793670400977135,\n",
       "   0.22462216019630432,\n",
       "   0.07627135515213013,\n",
       "   0.04514884203672409,\n",
       "   0.0584084689617157,\n",
       "   0.10929856449365616,\n",
       "   0.035253044217824936,\n",
       "   0.01559625193476677,\n",
       "   0.013577163219451904],\n",
       "  [0.007228714879602194,\n",
       "   0.0060742199420928955,\n",
       "   0.011056041345000267,\n",
       "   0.00900836382061243,\n",
       "   0.07774664461612701,\n",
       "   0.21826252341270447,\n",
       "   0.035505931824445724,\n",
       "   0.23809383809566498,\n",
       "   0.2266567349433899,\n",
       "   0.08492054045200348,\n",
       "   0.01709878444671631,\n",
       "   0.0397634282708168,\n",
       "   0.0167390089482069,\n",
       "   0.0063324240036308765,\n",
       "   0.005512842908501625],\n",
       "  [0.0035782784689217806,\n",
       "   0.005359055008739233,\n",
       "   0.004149807151407003,\n",
       "   0.0055876621045172215,\n",
       "   0.008408895693719387,\n",
       "   0.012961878441274166,\n",
       "   0.17214486002922058,\n",
       "   0.027425561100244522,\n",
       "   0.014880097471177578,\n",
       "   0.7064885497093201,\n",
       "   0.007852272130548954,\n",
       "   0.009274972602725029,\n",
       "   0.014600914902985096,\n",
       "   0.003628138452768326,\n",
       "   0.0036590411327779293],\n",
       "  [0.007103194482624531,\n",
       "   0.007891187444329262,\n",
       "   0.008725148625671864,\n",
       "   0.054802581667900085,\n",
       "   0.04579288512468338,\n",
       "   0.03368080407381058,\n",
       "   0.02679453045129776,\n",
       "   0.04378972575068474,\n",
       "   0.0200247373431921,\n",
       "   0.02852955460548401,\n",
       "   0.34585878252983093,\n",
       "   0.11289872974157333,\n",
       "   0.24754595756530762,\n",
       "   0.009676819667220116,\n",
       "   0.006885323207825422],\n",
       "  [0.0065566846169531345,\n",
       "   0.007946044206619263,\n",
       "   0.0075373295694589615,\n",
       "   0.007862917147576809,\n",
       "   0.015765480697155,\n",
       "   0.03393528610467911,\n",
       "   0.02739201858639717,\n",
       "   0.011213157325983047,\n",
       "   0.014669311232864857,\n",
       "   0.33705487847328186,\n",
       "   0.05606571584939957,\n",
       "   0.029029978439211845,\n",
       "   0.43060502409935,\n",
       "   0.007562020793557167,\n",
       "   0.00680405693128705],\n",
       "  [0.00604318268597126,\n",
       "   0.00882710050791502,\n",
       "   0.00583982327952981,\n",
       "   0.018451131880283356,\n",
       "   0.012412630952894688,\n",
       "   0.015154428780078888,\n",
       "   0.022438034415245056,\n",
       "   0.011593696661293507,\n",
       "   0.01217340212315321,\n",
       "   0.10061518102884293,\n",
       "   0.0314955934882164,\n",
       "   0.019623102620244026,\n",
       "   0.7268270254135132,\n",
       "   0.0042776064947247505,\n",
       "   0.004228129982948303],\n",
       "  [0.05502630025148392,\n",
       "   0.048057641834020615,\n",
       "   0.05371672287583351,\n",
       "   0.06345729529857635,\n",
       "   0.07389234751462936,\n",
       "   0.07568860054016113,\n",
       "   0.052783865481615067,\n",
       "   0.06770184636116028,\n",
       "   0.06341211497783661,\n",
       "   0.05225980281829834,\n",
       "   0.1231694370508194,\n",
       "   0.09874189645051956,\n",
       "   0.06106431409716606,\n",
       "   0.06320204585790634,\n",
       "   0.04782582074403763],\n",
       "  [0.06751631200313568,\n",
       "   0.06632179766893387,\n",
       "   0.06473962217569351,\n",
       "   0.06741329282522202,\n",
       "   0.06538397073745728,\n",
       "   0.0662652850151062,\n",
       "   0.06368929892778397,\n",
       "   0.06451314687728882,\n",
       "   0.0660991445183754,\n",
       "   0.06830303370952606,\n",
       "   0.07378979027271271,\n",
       "   0.06841585785150528,\n",
       "   0.06605063378810883,\n",
       "   0.06592094898223877,\n",
       "   0.06557781249284744]],\n",
       " 'premise_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'wandering',\n",
       "  'along',\n",
       "  'the',\n",
       "  'shore',\n",
       "  'drinking',\n",
       "  'iced',\n",
       "  'tea',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'hypothesis_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'a',\n",
       "  'blanket',\n",
       "  'near',\n",
       "  'some',\n",
       "  'rocks',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'politics',\n",
       "  '.',\n",
       "  '@@NULL@@']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor.predict(\n",
    "  hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\",\n",
    "  premise=\"Two women are wandering along the shore drinking iced tea.\"\n",
    ")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction['premise_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral', 'e+c'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "#     entailment, contradiction, neutral = None\n",
    "    for hypothesis in doc.sents:\n",
    "        if (premise != hypothesis):\n",
    "            prediction = predictor.predict(hypothesis=hypothesis.text, premise=premise.text)\n",
    "            entailment, contradiction, neutral = prediction['label_probs']\n",
    "            results.loc[i] = [premise.text, hypothesis.text, entailment, contradiction, neutral, (entailment + (1 - contradiction)) / 2]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "      <th>e+c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.956455</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.977611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.908410</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.904193</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.925338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.912021</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.162772</td>\n",
       "      <td>0.917696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>0.911948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.781964</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.216260</td>\n",
       "      <td>0.890094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.755611</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.236309</td>\n",
       "      <td>0.873766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.767259</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.211775</td>\n",
       "      <td>0.873146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.739140</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.248626</td>\n",
       "      <td>0.863452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.271786</td>\n",
       "      <td>0.861288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.735134</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>0.854201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.727070</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.250334</td>\n",
       "      <td>0.852237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.717547</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.246891</td>\n",
       "      <td>0.840992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.678346</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.832578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.659936</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.289775</td>\n",
       "      <td>0.804823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.604333</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.389258</td>\n",
       "      <td>0.798962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.682849</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.228379</td>\n",
       "      <td>0.797039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.762202</td>\n",
       "      <td>0.184278</td>\n",
       "      <td>0.053520</td>\n",
       "      <td>0.788962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.429359</td>\n",
       "      <td>0.781854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.584167</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.776704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>0.566095</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.772994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.589310</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.772807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>0.571464</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.390039</td>\n",
       "      <td>0.766483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.763730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.538478</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>0.408755</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.715452</td>\n",
       "      <td>0.238567</td>\n",
       "      <td>0.045980</td>\n",
       "      <td>0.738443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.548609</td>\n",
       "      <td>0.073129</td>\n",
       "      <td>0.378262</td>\n",
       "      <td>0.737740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.474978</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.461011</td>\n",
       "      <td>0.705483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>0.458307</td>\n",
       "      <td>0.689153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.523283</td>\n",
       "      <td>0.235219</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.644032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.381097</td>\n",
       "      <td>0.175689</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>0.602704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.373584</td>\n",
       "      <td>0.176693</td>\n",
       "      <td>0.449723</td>\n",
       "      <td>0.598446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.386267</td>\n",
       "      <td>0.217766</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.584250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>0.298593</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.515255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.269745</td>\n",
       "      <td>0.259522</td>\n",
       "      <td>0.470732</td>\n",
       "      <td>0.505112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.285518</td>\n",
       "      <td>0.384351</td>\n",
       "      <td>0.330131</td>\n",
       "      <td>0.450584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.234756</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>0.426192</td>\n",
       "      <td>0.447852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.352285</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.447454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.268840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>0.073138</td>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.265046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>0.526356</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.246635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.573239</td>\n",
       "      <td>0.388022</td>\n",
       "      <td>0.232750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>0.102812</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.185157</td>\n",
       "      <td>0.195390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.344303</td>\n",
       "      <td>0.193247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I dunno.</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.894391</td>\n",
       "      <td>0.104787</td>\n",
       "      <td>0.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.925812</td>\n",
       "      <td>0.071030</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "43   But I'm not totally sure what I should be doing?   \n",
       "44   But I'm not totally sure what I should be doing?   \n",
       "1                   I guess I am feeling kinda tired.   \n",
       "34   I feel stressed certainly, too much to do maybe?   \n",
       "8            I feel overwhelmed, a bit, maybe hungry.   \n",
       "9            I feel overwhelmed, a bit, maybe hungry.   \n",
       "32   I feel stressed certainly, too much to do maybe?   \n",
       "35   I feel stressed certainly, too much to do maybe?   \n",
       "27  I find myself wanting something, but I'm not s...   \n",
       "47   But I'm not totally sure what I should be doing?   \n",
       "40   But I'm not totally sure what I should be doing?   \n",
       "0                   I guess I am feeling kinda tired.   \n",
       "31  I find myself wanting something, but I'm not s...   \n",
       "41   But I'm not totally sure what I should be doing?   \n",
       "10           I feel overwhelmed, a bit, maybe hungry.   \n",
       "11           I feel overwhelmed, a bit, maybe hungry.   \n",
       "29  I find myself wanting something, but I'm not s...   \n",
       "3                   I guess I am feeling kinda tired.   \n",
       "39   I feel stressed certainly, too much to do maybe?   \n",
       "42   But I'm not totally sure what I should be doing?   \n",
       "33   I feel stressed certainly, too much to do maybe?   \n",
       "67     but a part of me wants to stay up, nonetheless   \n",
       "24  I find myself wanting something, but I'm not s...   \n",
       "36   I feel stressed certainly, too much to do maybe?   \n",
       "25  I find myself wanting something, but I'm not s...   \n",
       "70     but a part of me wants to stay up, nonetheless   \n",
       "68     but a part of me wants to stay up, nonetheless   \n",
       "26  I find myself wanting something, but I'm not s...   \n",
       "37   I feel stressed certainly, too much to do maybe?   \n",
       "2                   I guess I am feeling kinda tired.   \n",
       "28  I find myself wanting something, but I'm not s...   \n",
       "63       and it's really time for me to get to bed...   \n",
       "45   But I'm not totally sure what I should be doing?   \n",
       "12           I feel overwhelmed, a bit, maybe hungry.   \n",
       "55                               Now it's a lot later   \n",
       "15           I feel overwhelmed, a bit, maybe hungry.   \n",
       "4                   I guess I am feeling kinda tired.   \n",
       "66     but a part of me wants to stay up, nonetheless   \n",
       "59       and it's really time for me to get to bed...   \n",
       "51                               Now it's a lot later   \n",
       "30  I find myself wanting something, but I'm not s...   \n",
       "53                               Now it's a lot later   \n",
       "61       and it's really time for me to get to bed...   \n",
       "46   But I'm not totally sure what I should be doing?   \n",
       "21                                           I dunno.   \n",
       "60       and it's really time for me to get to bed...   \n",
       "50                               Now it's a lot later   \n",
       "58       and it's really time for me to get to bed...   \n",
       "22                                           I dunno.   \n",
       "6                   I guess I am feeling kinda tired.   \n",
       "\n",
       "                                           hypothesis  entailment  \\\n",
       "43  I find myself wanting something, but I'm not s...    0.956455   \n",
       "44   I feel stressed certainly, too much to do maybe?    0.908410   \n",
       "1                                            I dunno.    0.904193   \n",
       "34                                           I dunno.    0.912021   \n",
       "8                   I guess I am feeling kinda tired.    0.836310   \n",
       "9                                            I dunno.    0.867386   \n",
       "32                  I guess I am feeling kinda tired.    0.781964   \n",
       "35  I find myself wanting something, but I'm not s...    0.755611   \n",
       "27   I feel stressed certainly, too much to do maybe?    0.767259   \n",
       "47     but a part of me wants to stay up, nonetheless    0.768105   \n",
       "40                  I guess I am feeling kinda tired.    0.739140   \n",
       "0            I feel overwhelmed, a bit, maybe hungry.    0.725396   \n",
       "31     but a part of me wants to stay up, nonetheless    0.735134   \n",
       "41           I feel overwhelmed, a bit, maybe hungry.    0.727070   \n",
       "10  I find myself wanting something, but I'm not s...    0.717547   \n",
       "11   I feel stressed certainly, too much to do maybe?    0.678346   \n",
       "29                               Now it's a lot later    0.659936   \n",
       "3    I feel stressed certainly, too much to do maybe?    0.604333   \n",
       "39     but a part of me wants to stay up, nonetheless    0.682849   \n",
       "42                                           I dunno.    0.762202   \n",
       "33           I feel overwhelmed, a bit, maybe hungry.    0.567174   \n",
       "67  I find myself wanting something, but I'm not s...    0.584167   \n",
       "24                  I guess I am feeling kinda tired.    0.566095   \n",
       "36   But I'm not totally sure what I should be doing?    0.589310   \n",
       "25           I feel overwhelmed, a bit, maybe hungry.    0.571464   \n",
       "70                               Now it's a lot later    0.566257   \n",
       "68   I feel stressed certainly, too much to do maybe?    0.538478   \n",
       "26                                           I dunno.    0.715452   \n",
       "37                               Now it's a lot later    0.548609   \n",
       "2   I find myself wanting something, but I'm not s...    0.474978   \n",
       "28   But I'm not totally sure what I should be doing?    0.460000   \n",
       "63     but a part of me wants to stay up, nonetheless    0.523283   \n",
       "45                               Now it's a lot later    0.381097   \n",
       "12   But I'm not totally sure what I should be doing?    0.373584   \n",
       "55     but a part of me wants to stay up, nonetheless    0.386267   \n",
       "15     but a part of me wants to stay up, nonetheless    0.329103   \n",
       "4    But I'm not totally sure what I should be doing?    0.269745   \n",
       "66                                           I dunno.    0.285518   \n",
       "59  I find myself wanting something, but I'm not s...    0.234756   \n",
       "51  I find myself wanting something, but I'm not s...    0.247193   \n",
       "30       and it's really time for me to get to bed...    0.033333   \n",
       "53   But I'm not totally sure what I should be doing?    0.045921   \n",
       "61   But I'm not totally sure what I should be doing?    0.073138   \n",
       "46       and it's really time for me to get to bed...    0.019627   \n",
       "21                               Now it's a lot later    0.038739   \n",
       "60   I feel stressed certainly, too much to do maybe?    0.102812   \n",
       "50                                           I dunno.    0.021096   \n",
       "58                                           I dunno.    0.018045   \n",
       "22       and it's really time for me to get to bed...    0.000822   \n",
       "6        and it's really time for me to get to bed...    0.003158   \n",
       "\n",
       "    contradiction   neutral       e+c  \n",
       "43       0.001234  0.042311  0.977611  \n",
       "44       0.005447  0.086143  0.951481  \n",
       "1        0.053517  0.042290  0.925338  \n",
       "34       0.070635  0.017344  0.920693  \n",
       "8        0.000918  0.162772  0.917696  \n",
       "9        0.043489  0.089125  0.911948  \n",
       "32       0.001776  0.216260  0.890094  \n",
       "35       0.008080  0.236309  0.873766  \n",
       "27       0.020967  0.211775  0.873146  \n",
       "47       0.025418  0.206478  0.871344  \n",
       "40       0.012235  0.248626  0.863452  \n",
       "0        0.002819  0.271786  0.861288  \n",
       "31       0.026731  0.238136  0.854201  \n",
       "41       0.022596  0.250334  0.852237  \n",
       "10       0.035562  0.246891  0.840992  \n",
       "11       0.013190  0.308464  0.832578  \n",
       "29       0.050289  0.289775  0.804823  \n",
       "3        0.006409  0.389258  0.798962  \n",
       "39       0.088772  0.228379  0.797039  \n",
       "42       0.184278  0.053520  0.788962  \n",
       "33       0.003467  0.429359  0.781854  \n",
       "67       0.030758  0.385075  0.776704  \n",
       "24       0.020107  0.413798  0.772994  \n",
       "36       0.043696  0.366994  0.772807  \n",
       "25       0.038498  0.390039  0.766483  \n",
       "70       0.038796  0.394947  0.763730  \n",
       "68       0.052767  0.408755  0.742856  \n",
       "26       0.238567  0.045980  0.738443  \n",
       "37       0.073129  0.378262  0.737740  \n",
       "2        0.064011  0.461011  0.705483  \n",
       "28       0.081693  0.458307  0.689153  \n",
       "63       0.235219  0.241498  0.644032  \n",
       "45       0.175689  0.443214  0.602704  \n",
       "12       0.176693  0.449723  0.598446  \n",
       "55       0.217766  0.395967  0.584250  \n",
       "15       0.298593  0.372304  0.515255  \n",
       "4        0.259522  0.470732  0.505112  \n",
       "66       0.384351  0.330131  0.450584  \n",
       "59       0.339052  0.426192  0.447852  \n",
       "51       0.352285  0.400522  0.447454  \n",
       "30       0.486933  0.479734  0.273200  \n",
       "53       0.508242  0.445838  0.268840  \n",
       "61       0.543047  0.383816  0.265046  \n",
       "46       0.526356  0.454018  0.246635  \n",
       "21       0.573239  0.388022  0.232750  \n",
       "60       0.712032  0.185157  0.195390  \n",
       "50       0.634602  0.344303  0.193247  \n",
       "58       0.898486  0.083470  0.059780  \n",
       "22       0.894391  0.104787  0.053215  \n",
       "6        0.925812  0.071030  0.038673  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='e+c', ascending=False).loc[results['neutral'] < .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'I feel stressed'\n",
    "\n",
    "results = pd.DataFrame([], columns=['premise', 'hypothesis', 'entailment', 'contradiction', 'neutral'])\n",
    "i = 0\n",
    "for premise in doc.sents:\n",
    "    prediction = predictor.predict(hypothesis=hypothesis, premise=premise.text)\n",
    "    entailment, contradiction, neutral = prediction['label_probs']\n",
    "    results.loc[i] = [premise.text, hypothesis, entailment, contradiction, neutral]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailment</th>\n",
       "      <th>contradiction</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel stressed certainly, too much to do maybe?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.985132</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.014467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess I am feeling kinda tired.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.060882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I feel overwhelmed, a bit, maybe hungry.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.933847</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.063966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I find myself wanting something, but I'm not s...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.833155</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.162525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>But I'm not totally sure what I should be doing?</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.769592</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.189401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dunno.</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.493208</td>\n",
       "      <td>0.287141</td>\n",
       "      <td>0.219651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>but a part of me wants to stay up, nonetheless</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.115519</td>\n",
       "      <td>0.591396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and it's really time for me to get to bed...</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.537717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Now it's a lot later</td>\n",
       "      <td>I feel stressed</td>\n",
       "      <td>0.081763</td>\n",
       "      <td>0.259905</td>\n",
       "      <td>0.658333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise       hypothesis  \\\n",
       "4   I feel stressed certainly, too much to do maybe?  I feel stressed   \n",
       "0                  I guess I am feeling kinda tired.  I feel stressed   \n",
       "1           I feel overwhelmed, a bit, maybe hungry.  I feel stressed   \n",
       "3  I find myself wanting something, but I'm not s...  I feel stressed   \n",
       "5   But I'm not totally sure what I should be doing?  I feel stressed   \n",
       "2                                           I dunno.  I feel stressed   \n",
       "8     but a part of me wants to stay up, nonetheless  I feel stressed   \n",
       "7       and it's really time for me to get to bed...  I feel stressed   \n",
       "6                               Now it's a lot later  I feel stressed   \n",
       "\n",
       "   entailment  contradiction   neutral  \n",
       "4    0.985132       0.000401  0.014467  \n",
       "0    0.936851       0.002266  0.060882  \n",
       "1    0.933847       0.002187  0.063966  \n",
       "3    0.833155       0.004319  0.162525  \n",
       "5    0.769592       0.041008  0.189401  \n",
       "2    0.493208       0.287141  0.219651  \n",
       "8    0.293085       0.115519  0.591396  \n",
       "7    0.109000       0.353283  0.537717  \n",
       "6    0.081763       0.259905  0.658333  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='entailment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(shape):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    nlp.add_pipe(KerasSimilarityShim.load(nlp.path / 'similarity', nlp, shape[0]))\n",
    "\n",
    "    doc1 = nlp(u'The king of France is bald.')\n",
    "    doc2 = nlp(u'France has no king.')\n",
    "\n",
    "    print(\"Sentence 1:\", doc1)\n",
    "    print(\"Sentence 2:\", doc2)\n",
    "\n",
    "    entailment_type, confidence = doc1.similarity(doc2)\n",
    "    print(\"Entailment type:\", entailment_type, \"(Confidence:\", confidence, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    "vectorizer = Vectorizer(\n",
    "    tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "    min_df=3, max_df=0.95, max_n_terms=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=20)\n",
    "model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.17393909018065556),\n",
       " ('overwhelmed', 0.1291297332498814),\n",
       " ('time', 0.12848449534695075),\n",
       " ('bit', 0.12152651186559832),\n",
       " ('lot', 0.12055447304217964),\n",
       " ('hungry', 0.11759733293982846),\n",
       " ('tired', 0.07154722366605952),\n",
       " ('bed', 0.0712721534037857),\n",
       " ('stressed', 0.06594898630506062)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sure', 0.33850825516726607),\n",
       " ('stressed', 0.1517767729454664),\n",
       " ('bed', 0.1484936700017998),\n",
       " ('time', 0.0968496495027204),\n",
       " ('lot', 0.07202910699164278),\n",
       " ('hungry', 0.07058674833731196),\n",
       " ('bit', 0.051420957354160426),\n",
       " ('overwhelmed', 0.03964374936267677),\n",
       " ('tired', 0.030691090336955648)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = textacy.keyterms.sgrank(doc)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I guess I am feeling kinda tired. I feel overwhelmed, a bit, maybe hungry. I dunno. I find myself wanting something, but I'm not sure what it is. I feel stressed certainly, too much to do maybe? But I'm not totally sure what I should be doing? Now it's a lot later and it's really time for me to get to bed...but a part of me wants to stay up, nonetheless\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.lexicon_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2018 21:16:22 - INFO - textacy.lexicon_methods -   Downloaded DepecheMood (4MB) from https://github.com/marcoguerini/DepecheMood/releases/download/v1.0/DepecheMood_V1.0.zip and wrote it to data\n"
     ]
    }
   ],
   "source": [
    "textacy.lexicon_methods.download_depechemood(data_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'AFRAID': 0.1103335419756097,\n",
       "             'AMUSED': 0.14808456209756102,\n",
       "             'ANGRY': 0.10694153351219511,\n",
       "             'ANNOYED': 0.12443051617073168,\n",
       "             'DONT_CARE': 0.13096818899999998,\n",
       "             'HAPPY': 0.11531756726829266,\n",
       "             'INSPIRED': 0.14843126431707318,\n",
       "             'SAD': 0.11549282553658531})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.lexicon_methods.emotional_valence(words=[word for word in doc], dm_data_dir='data/DepecheMood_V1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   loading archive file data/event2mind.tar.gz\n",
      "12/06/2018 17:58:27 - INFO - event2mind_hack -   extracting archive file data/event2mind.tar.gz to temp dir /tmp/tmp0dlhchct\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   vocabulary.type = default\n",
      "12/06/2018 17:58:28 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmp0dlhchct/vocabulary.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Model'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens', 'type': 'event2mind'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'event2mind_hack.Event2Mind'> from params {'embedding_dropout': 0.2, 'encoder': {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'}, 'max_decoding_steps': 10, 'source_embedder': {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}}, 'target_namespace': 'target_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}}} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.type = basic\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.embedder_to_indexer_map = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.allow_unmatched_keys = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.type = embedding\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.vocab_namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.embedding_dim = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.projection_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.trainable = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.padding_index = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.max_norm = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.source_embedder.token_embedders.tokens.sparse = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.embedding_dropout = 0.2\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': True, 'hidden_size': 50, 'input_size': 300, 'num_layers': 1, 'type': 'gru'} and extras {'vocab': <allennlp.data.vocabulary.Vocabulary object at 0x7fccd4e752e8>}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.type = gru\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.hidden_size = 50\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.input_size = 300\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.num_layers = 1\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.max_decoding_steps = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.beam_size = 10\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_names = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   model.target_embedding_dim = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}, 'type': 'event2mind'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.type = event2mind\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.event2mind.Event2MindDatasetReader'> from params {'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word', 'word_splitter': {'type': 'spacy'}}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens'}}, 'target_tokenizer': {'type': 'word'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'type': 'spacy'}} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'type': 'spacy'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.type = spacy\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.language = en_core_web_sm\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.pos_tags = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.parse = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.word_splitter.ner = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.type = word\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_tokenizer.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'source_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.type = single_id\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'namespace': 'target_tokens'} and extras {}\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.source_add_start_token = True\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.dummy_instances_for_vocab_generation = False\n",
      "12/06/2018 17:58:28 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xintent_top_k_predictions': [[4, 3, 3, 3],\n",
       "  [684, 3, 3, 3],\n",
       "  [323, 3, 3, 3],\n",
       "  [282, 3, 3, 3],\n",
       "  [255, 3, 3, 3],\n",
       "  [153, 229, 3, 3],\n",
       "  [13, 175, 3, 3],\n",
       "  [44, 251, 3, 3],\n",
       "  [13, 267, 3, 3],\n",
       "  [211, 32, 80, 3]],\n",
       " 'xintent_top_k_log_probabilities': [-1.128340721130371,\n",
       "  -4.298313617706299,\n",
       "  -4.499514579772949,\n",
       "  -4.616389751434326,\n",
       "  -4.652942657470703,\n",
       "  -5.256786823272705,\n",
       "  -5.543419361114502,\n",
       "  -6.178347587585449,\n",
       "  -6.431229114532471,\n",
       "  -9.508707046508789],\n",
       " 'xreact_top_k_predictions': [[54, 3],\n",
       "  [70, 3],\n",
       "  [53, 3],\n",
       "  [109, 3],\n",
       "  [5, 3],\n",
       "  [73, 3],\n",
       "  [11, 3],\n",
       "  [92, 3],\n",
       "  [25, 3],\n",
       "  [63, 3]],\n",
       " 'xreact_top_k_log_probabilities': [-3.147449016571045,\n",
       "  -3.159241199493408,\n",
       "  -3.1768059730529785,\n",
       "  -3.2743079662323,\n",
       "  -3.3373990058898926,\n",
       "  -3.516559362411499,\n",
       "  -3.596619129180908,\n",
       "  -3.8112082481384277,\n",
       "  -3.8679120540618896,\n",
       "  -3.9374184608459473],\n",
       " 'oreact_top_k_predictions': [[4, 3],\n",
       "  [63, 3],\n",
       "  [36, 3],\n",
       "  [89, 3],\n",
       "  [83, 3],\n",
       "  [91, 3],\n",
       "  [138, 3],\n",
       "  [53, 3],\n",
       "  [92, 3],\n",
       "  [5, 3]],\n",
       " 'oreact_top_k_log_probabilities': [-1.067413330078125,\n",
       "  -2.62384033203125,\n",
       "  -3.9126994609832764,\n",
       "  -3.979201316833496,\n",
       "  -4.0716962814331055,\n",
       "  -4.296855926513672,\n",
       "  -4.327937602996826,\n",
       "  -4.378903388977051,\n",
       "  -4.453375339508057,\n",
       "  -4.506431579589844],\n",
       " 'xintent_top_k_predicted_tokens': [['none'],\n",
       "  ['annoying'],\n",
       "  ['noticed'],\n",
       "  ['communicate'],\n",
       "  ['heard'],\n",
       "  ['express', 'anger'],\n",
       "  ['get', 'attention'],\n",
       "  ['show', 'affection'],\n",
       "  ['get', 'revenge'],\n",
       "  ['let', 'someone', 'know']],\n",
       " 'xreact_top_k_predicted_tokens': [['upset'],\n",
       "  ['worried'],\n",
       "  ['nervous'],\n",
       "  ['curious'],\n",
       "  ['happy'],\n",
       "  ['scared'],\n",
       "  ['satisfied'],\n",
       "  ['anxious'],\n",
       "  ['relieved'],\n",
       "  ['annoyed']],\n",
       " 'oreact_top_k_predicted_tokens': [['none'],\n",
       "  ['annoyed'],\n",
       "  ['angry'],\n",
       "  ['informed'],\n",
       "  ['surprised'],\n",
       "  ['interested'],\n",
       "  ['frustrated'],\n",
       "  ['nervous'],\n",
       "  ['anxious'],\n",
       "  ['happy']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from event2mind_hack import load_event2mind_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "archive = load_event2mind_archive('data/event2mind.tar.gz')\n",
    "predictor = Predictor.from_archive(archive)\n",
    "predictor.predict(\n",
    "  source=\"PersonX drops a hint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.128341</td>\n",
       "      <td>0.323570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoying]</td>\n",
       "      <td>-4.298314</td>\n",
       "      <td>0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noticed]</td>\n",
       "      <td>-4.499515</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[communicate]</td>\n",
       "      <td>-4.616390</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[heard]</td>\n",
       "      <td>-4.652943</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[express, anger]</td>\n",
       "      <td>-5.256787</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[get, attention]</td>\n",
       "      <td>-5.543419</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[show, affection]</td>\n",
       "      <td>-6.178348</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[get, revenge]</td>\n",
       "      <td>-6.431229</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[let, someone, know]</td>\n",
       "      <td>-9.508707</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tokens     p_log         p\n",
       "0                [none] -1.128341  0.323570\n",
       "1            [annoying] -4.298314  0.013591\n",
       "2             [noticed] -4.499515  0.011114\n",
       "3         [communicate] -4.616390  0.009888\n",
       "4               [heard] -4.652943  0.009534\n",
       "5      [express, anger] -5.256787  0.005212\n",
       "6      [get, attention] -5.543419  0.003913\n",
       "7     [show, affection] -6.178348  0.002074\n",
       "8        [get, revenge] -6.431229  0.001610\n",
       "9  [let, someone, know] -9.508707  0.000074"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xintent = pd.DataFrame({\n",
    "    'tokens': prediction['xintent_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xintent_top_k_log_probabilities']\n",
    "})\n",
    "xintent['p'] = xintent['p_log'].apply(math.exp)\n",
    "xintent.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upset]</td>\n",
       "      <td>-3.147449</td>\n",
       "      <td>0.042962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[worried]</td>\n",
       "      <td>-3.159241</td>\n",
       "      <td>0.042458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-3.176806</td>\n",
       "      <td>0.041719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[curious]</td>\n",
       "      <td>-3.274308</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-3.337399</td>\n",
       "      <td>0.035529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[scared]</td>\n",
       "      <td>-3.516559</td>\n",
       "      <td>0.029701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[satisfied]</td>\n",
       "      <td>-3.596619</td>\n",
       "      <td>0.027416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-3.811208</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[relieved]</td>\n",
       "      <td>-3.867912</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-3.937418</td>\n",
       "      <td>0.019498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens     p_log         p\n",
       "0      [upset] -3.147449  0.042962\n",
       "1    [worried] -3.159241  0.042458\n",
       "2    [nervous] -3.176806  0.041719\n",
       "3    [curious] -3.274308  0.037843\n",
       "4      [happy] -3.337399  0.035529\n",
       "5     [scared] -3.516559  0.029701\n",
       "6  [satisfied] -3.596619  0.027416\n",
       "7    [anxious] -3.811208  0.022121\n",
       "8   [relieved] -3.867912  0.020902\n",
       "9    [annoyed] -3.937418  0.019498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreact = pd.DataFrame({\n",
    "    'tokens': prediction['xreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['xreact_top_k_log_probabilities']\n",
    "})\n",
    "xreact['p'] = xreact['p_log'].apply(math.exp)\n",
    "xreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_log</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[none]</td>\n",
       "      <td>-1.067413</td>\n",
       "      <td>0.343897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[annoyed]</td>\n",
       "      <td>-2.623840</td>\n",
       "      <td>0.072524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[angry]</td>\n",
       "      <td>-3.912699</td>\n",
       "      <td>0.019986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[informed]</td>\n",
       "      <td>-3.979201</td>\n",
       "      <td>0.018701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surprised]</td>\n",
       "      <td>-4.071696</td>\n",
       "      <td>0.017048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[interested]</td>\n",
       "      <td>-4.296856</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[frustrated]</td>\n",
       "      <td>-4.327938</td>\n",
       "      <td>0.013195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[nervous]</td>\n",
       "      <td>-4.378903</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[anxious]</td>\n",
       "      <td>-4.453375</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[happy]</td>\n",
       "      <td>-4.506432</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens     p_log         p\n",
       "0        [none] -1.067413  0.343897\n",
       "1     [annoyed] -2.623840  0.072524\n",
       "2       [angry] -3.912699  0.019986\n",
       "3    [informed] -3.979201  0.018701\n",
       "4   [surprised] -4.071696  0.017048\n",
       "5  [interested] -4.296856  0.013611\n",
       "6  [frustrated] -4.327938  0.013195\n",
       "7     [nervous] -4.378903  0.012539\n",
       "8     [anxious] -4.453375  0.011639\n",
       "9       [happy] -4.506432  0.011038"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oreact = pd.DataFrame({\n",
    "    'tokens': prediction['oreact_top_k_predicted_tokens'],\n",
    "    'p_log': prediction['oreact_top_k_log_probabilities']\n",
    "})\n",
    "oreact['p'] = oreact['p_log'].apply(math.exp)\n",
    "oreact.sort_values(by='p', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
